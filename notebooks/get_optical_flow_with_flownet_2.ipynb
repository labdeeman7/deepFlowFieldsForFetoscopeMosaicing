{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"get_optical_flow_with_flownet_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNobYACPgzsuU/gbtBCwagb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3tDQdXaknUfl"},"source":["To use this, you need to download the desired flownet pre-trained model weights(flownet2 for this project) from https://github.com/NVIDIA/flownet2-pytorch and keep it in a google drive folder. \n","\n","You can also download it directly in colab, or on your system, but the flownet model with pretrained weights are transferred by google drive file share which might make this harder."]},{"cell_type":"code","metadata":{"id":"kw_f76igD0Rn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297212540,"user_tz":-60,"elapsed":2459,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"5ec1ddc8-b284-4ef2-b85b-ccf9e08466ca"},"source":["#pull github code\n","!git clone https://github.com/NVIDIA/flownet2-pytorch.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'flownet2-pytorch'...\n","remote: Enumerating objects: 557, done.\u001b[K\n","remote: Total 557 (delta 0), reused 0 (delta 0), pack-reused 557\u001b[K\n","Receiving objects: 100% (557/557), 6.28 MiB | 10.70 MiB/s, done.\n","Resolving deltas: 100% (312/312), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dTY98g4S7H5d"},"source":["After cloning the repo, for colab, you need to perform these changes \n","\n","In setup.py in /content/flownet2-pytorch/networks/(channelnorm_package, correlation_package, and resample_2d_package) \n","1. change cxx_args = ['-std=c++11'] to cxx_args = ['-std=c++14'] \n","2. Add '-gencode', 'arch=compute_37,code=sm_37' to nvcc-args \n","\n"]},{"cell_type":"code","metadata":{"id":"a9yZeFCl7dIS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297282688,"user_tz":-60,"elapsed":573,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"ca36c457-ad60-4c9b-ecdb-43b81e1df4b7"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sun Feb 14 10:08:03 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fKyDhPgj95Fs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297292979,"user_tz":-60,"elapsed":8280,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"932098d6-e717-40f3-85c6-0171d0f1de9b"},"source":["#install tensorboardX, setproctitle, colorama\n","!pip install tensorboardX\n","!pip install setproctitle\n","!pip install colorama"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\r\u001b[K     |█                               | 10kB 21.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 18.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 11.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (53.0.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.1\n","Collecting setproctitle\n","  Downloading https://files.pythonhosted.org/packages/bb/7e/9b3683a42e3aa7cba8364149fd12be1a17c8d0b5cf3f587320b86e5e0248/setproctitle-1.2.2-cp36-cp36m-manylinux1_x86_64.whl\n","Installing collected packages: setproctitle\n","Successfully installed setproctitle-1.2.2\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qkUa7LwZe3ps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297305665,"user_tz":-60,"elapsed":18911,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"c8c82030-1388-4260-e1e8-f754c1c1be7c"},"source":["!pip install scipy==1.1.0\n","!pip install Pillow"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting scipy==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n","\u001b[K     |████████████████████████████████| 31.2MB 110kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.19.5)\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scipy\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","Successfully installed scipy-1.1.0\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wOOtEdFALWy3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297305666,"user_tz":-60,"elapsed":17190,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"17f70624-f155-4868-b91d-0c769c6f77e6"},"source":["%cd ./flownet2-pytorch/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/flownet2-pytorch\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LZaz2ZqliT1B"},"source":["Then install"]},{"cell_type":"code","metadata":{"id":"-HxiF1ylMDNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297483767,"user_tz":-60,"elapsed":193911,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"027630ee-7e35-4398-8aba-c606aca904e4"},"source":["!bash install.sh"],"execution_count":6,"outputs":[{"output_type":"stream","text":["running install\n","running bdist_egg\n","running egg_info\n","creating correlation_cuda.egg-info\n","writing correlation_cuda.egg-info/PKG-INFO\n","writing dependency_links to correlation_cuda.egg-info/dependency_links.txt\n","writing top-level names to correlation_cuda.egg-info/top_level.txt\n","writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n","/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_ext\n","building 'correlation_cuda' extension\n","creating build\n","creating build/temp.linux-x86_64-3.6\n","x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c correlation_cuda.cc -o build/temp.linux-x86_64-3.6/correlation_cuda.o -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n","In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/torch.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[Kcorrelation_cuda.cc:1\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n"," #pragma omp parallel for if ((end - begin) >= grain_size)\n"," \n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c correlation_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/correlation_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_37,code=sm_37 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input\u001b[01;35m\u001b[K1\u001b[m\u001b[K.type(), \"channels_first_fwd_1\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input\u001b[01;35m\u001b[K2\u001b[m\u001b[K.type(), \"channels_first_fwd_2\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input\u001b[01;35m\u001b[K1\u001b[m\u001b[K.type(), \"correlation_forward\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:328:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:400:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:469:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:748:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:819:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:887:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1177:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1252:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1324:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt1.type(), \"lltm_forward_cuda\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt2.type(), \"lltm_forward_cuda\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(i\u001b[01;35m\u001b[Kn\u001b[m\u001b[Kput2.type(), \"lltm_forward_cuda\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:343:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:415:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:487:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:781:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:852:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:923:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1228:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1303:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1378:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rI\u001b[01;35m\u001b[Kn\u001b[m\u001b[Kput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:344:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:416:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:488:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:782:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:853:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:924:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1229:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1304:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1379:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","creating build/lib.linux-x86_64-3.6\n","x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/correlation_cuda.o build/temp.linux-x86_64-3.6/correlation_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/correlation_cuda.cpython-36m-x86_64-linux-gnu.so\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","copying build/lib.linux-x86_64-3.6/correlation_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n","creating stub loader for correlation_cuda.cpython-36m-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/correlation_cuda.py to correlation_cuda.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying correlation_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying correlation_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying correlation_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying correlation_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","zip_safe flag not set; analyzing archive contents...\n","__pycache__.correlation_cuda.cpython-36: module references __file__\n","creating dist\n","creating 'dist/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n","creating /root/.local/lib/python3.6/site-packages/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n","Extracting correlation_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n","Adding correlation-cuda 0.0.0 to easy-install.pth file\n","\n","Installed /root/.local/lib/python3.6/site-packages/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n","Processing dependencies for correlation-cuda==0.0.0\n","Finished processing dependencies for correlation-cuda==0.0.0\n","running install\n","running bdist_egg\n","running egg_info\n","creating resample2d_cuda.egg-info\n","writing resample2d_cuda.egg-info/PKG-INFO\n","writing dependency_links to resample2d_cuda.egg-info/dependency_links.txt\n","writing top-level names to resample2d_cuda.egg-info/top_level.txt\n","writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n","/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_ext\n","building 'resample2d_cuda' extension\n","creating build\n","creating build/temp.linux-x86_64-3.6\n","x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c resample2d_cuda.cc -o build/temp.linux-x86_64-3.6/resample2d_cuda.o -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n","In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/torch.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[Kresample2d_cuda.cc:2\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n"," #pragma omp parallel for if ((end - begin) >= grain_size)\n"," \n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c resample2d_kernel.cu -o build/temp.linux-x86_64-3.6/resample2d_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_37,code=sm_37 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_forward(at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kresample2d_kernel.cu:221:173:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:221:225:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:221:277:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_backward(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kresample2d_kernel.cu:269:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:269:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:269:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:269:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:298:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:298:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:298:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kresample2d_kernel.cu:298:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n","                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","creating build/lib.linux-x86_64-3.6\n","x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/resample2d_cuda.o build/temp.linux-x86_64-3.6/resample2d_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/resample2d_cuda.cpython-36m-x86_64-linux-gnu.so\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","copying build/lib.linux-x86_64-3.6/resample2d_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n","creating stub loader for resample2d_cuda.cpython-36m-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/resample2d_cuda.py to resample2d_cuda.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying resample2d_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying resample2d_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying resample2d_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying resample2d_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","zip_safe flag not set; analyzing archive contents...\n","__pycache__.resample2d_cuda.cpython-36: module references __file__\n","creating dist\n","creating 'dist/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n","creating /root/.local/lib/python3.6/site-packages/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n","Extracting resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n","Adding resample2d-cuda 0.0.0 to easy-install.pth file\n","\n","Installed /root/.local/lib/python3.6/site-packages/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n","Processing dependencies for resample2d-cuda==0.0.0\n","Finished processing dependencies for resample2d-cuda==0.0.0\n","running install\n","running bdist_egg\n","running egg_info\n","creating channelnorm_cuda.egg-info\n","writing channelnorm_cuda.egg-info/PKG-INFO\n","writing dependency_links to channelnorm_cuda.egg-info/dependency_links.txt\n","writing top-level names to channelnorm_cuda.egg-info/top_level.txt\n","writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n","/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_ext\n","building 'channelnorm_cuda' extension\n","creating build\n","creating build/temp.linux-x86_64-3.6\n","x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c channelnorm_cuda.cc -o build/temp.linux-x86_64-3.6/channelnorm_cuda.o -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n","In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/torch.h:3\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[Kchannelnorm_cuda.cc:1\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n"," #pragma omp parallel for if ((end - begin) >= grain_size)\n"," \n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c channelnorm_kernel.cu -o build/temp.linux-x86_64-3.6/channelnorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_37,code=sm_37 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt1.type(), \"channelnorm_forward\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:366:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:421:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:717:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:771:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1078:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1136:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n"," \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:368:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:423:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:482:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:549:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:855:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:909:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:967:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1033:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1350:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1408:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1470:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1540:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:363:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n"," \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n"," \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","creating build/lib.linux-x86_64-3.6\n","x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/channelnorm_cuda.o build/temp.linux-x86_64-3.6/channelnorm_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","copying build/lib.linux-x86_64-3.6/channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n","creating stub loader for channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/channelnorm_cuda.py to channelnorm_cuda.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying channelnorm_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying channelnorm_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying channelnorm_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying channelnorm_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","zip_safe flag not set; analyzing archive contents...\n","__pycache__.channelnorm_cuda.cpython-36: module references __file__\n","creating dist\n","creating 'dist/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n","creating /root/.local/lib/python3.6/site-packages/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n","Extracting channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n","Adding channelnorm-cuda 0.0.0 to easy-install.pth file\n","\n","Installed /root/.local/lib/python3.6/site-packages/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n","Processing dependencies for channelnorm-cuda==0.0.0\n","Finished processing dependencies for channelnorm-cuda==0.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aK41Gd1vb87p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297644816,"user_tz":-60,"elapsed":550,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"daa1312d-3694-4009-99ed-da79b8eabe35"},"source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["convert.py\t\t  __init__.py\t    main.py\t   run-caffe2pytorch.sh\n","datasets.py\t\t  install.sh\t    models.py\t   utils\n","Dockerfile\t\t  launch_docker.sh  networks\n","download_caffe_models.sh  LICENSE\t    README.md\n","image.png\t\t  losses.py\t    run_a_pair.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rjCvrcAC6izw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297648791,"user_tz":-60,"elapsed":2158,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"4fb72bb4-b49f-429f-c744-9f0cf049a6f3"},"source":["!python main.py --help"],"execution_count":8,"outputs":[{"output_type":"stream","text":["usage: main.py [-h] [--start_epoch START_EPOCH] [--total_epochs TOTAL_EPOCHS]\n","               [--batch_size BATCH_SIZE] [--train_n_batches TRAIN_N_BATCHES]\n","               [--crop_size CROP_SIZE [CROP_SIZE ...]]\n","               [--gradient_clip GRADIENT_CLIP]\n","               [--schedule_lr_frequency SCHEDULE_LR_FREQUENCY]\n","               [--schedule_lr_fraction SCHEDULE_LR_FRACTION]\n","               [--rgb_max RGB_MAX] [--number_workers NUMBER_WORKERS]\n","               [--number_gpus NUMBER_GPUS] [--no_cuda] [--seed SEED]\n","               [--name NAME] [--save SAVE]\n","               [--validation_frequency VALIDATION_FREQUENCY]\n","               [--validation_n_batches VALIDATION_N_BATCHES]\n","               [--render_validation] [--inference] [--inference_visualize]\n","               [--inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]]\n","               [--inference_batch_size INFERENCE_BATCH_SIZE]\n","               [--inference_n_batches INFERENCE_N_BATCHES] [--save_flow]\n","               [--resume PATH] [--log_frequency LOG_FREQUENCY]\n","               [--skip_training] [--skip_validation] [--fp16]\n","               [--fp16_scale FP16_SCALE]\n","               [--model {ChannelNorm,FlowNet2,FlowNet2C,FlowNet2CS,FlowNet2CSS,FlowNet2S,FlowNet2SD,Resample2d,tofp16,tofp32}]\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --start_epoch START_EPOCH\n","  --total_epochs TOTAL_EPOCHS\n","  --batch_size BATCH_SIZE, -b BATCH_SIZE\n","                        Batch size\n","  --train_n_batches TRAIN_N_BATCHES\n","                        Number of min-batches per epoch. If < 0, it will be\n","                        determined by training_dataloader\n","  --crop_size CROP_SIZE [CROP_SIZE ...]\n","                        Spatial dimension to crop training samples for\n","                        training\n","  --gradient_clip GRADIENT_CLIP\n","  --schedule_lr_frequency SCHEDULE_LR_FREQUENCY\n","                        in number of iterations (0 for no schedule)\n","  --schedule_lr_fraction SCHEDULE_LR_FRACTION\n","  --rgb_max RGB_MAX\n","  --number_workers NUMBER_WORKERS, -nw NUMBER_WORKERS, --num_workers NUMBER_WORKERS\n","  --number_gpus NUMBER_GPUS, -ng NUMBER_GPUS\n","                        number of GPUs to use\n","  --no_cuda\n","  --seed SEED\n","  --name NAME           a name to append to the save directory\n","  --save SAVE, -s SAVE  directory for saving\n","  --validation_frequency VALIDATION_FREQUENCY\n","                        validate every n epochs\n","  --validation_n_batches VALIDATION_N_BATCHES\n","  --render_validation   run inference (save flows to file) and every\n","                        validation_frequency epoch\n","  --inference\n","  --inference_visualize\n","                        visualize the optical flow during inference\n","  --inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]\n","                        spatial size divisible by 64. default (-1,-1) -\n","                        largest possible valid size would be used\n","  --inference_batch_size INFERENCE_BATCH_SIZE\n","  --inference_n_batches INFERENCE_N_BATCHES\n","  --save_flow           save predicted flows to file\n","  --resume PATH         path to latest checkpoint (default: none)\n","  --log_frequency LOG_FREQUENCY, --summ_iter LOG_FREQUENCY\n","                        Log every n batches\n","  --skip_training\n","  --skip_validation\n","  --fp16                Run model in pseudo-fp16 mode (fp16 storage fp32\n","                        math).\n","  --fp16_scale FP16_SCALE\n","                        Loss scaling, positive power of 2 values can improve\n","                        fp16 convergence.\n","\n","Model:\n","  --model {ChannelNorm,FlowNet2,FlowNet2C,FlowNet2CS,FlowNet2CSS,FlowNet2S,FlowNet2SD,Resample2d,tofp16,tofp32}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AgzyDvlciYFs"},"source":["The pretrained weights are kept in a pretrained directory and the results are kept in a result directory"]},{"cell_type":"code","metadata":{"id":"0HKRjB0Hjxgb","executionInfo":{"status":"ok","timestamp":1613297652924,"user_tz":-60,"elapsed":831,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}}},"source":["!mkdir pretrained\n","!mkdir results"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7OkMWIiiogF"},"source":["Also I had the extracted consecutive frames of the fetoscopic dataset (videoXX) gotten from https://weiss-develop.cs.ucl.ac.uk/fetoscopy-data/fetoscopy-placenta-dataset/fetoscopy-placenta-dataset.zip and kept them on my google drive. \n","\n","Virtually all my data was on google drive hahaha, thank you google. "]},{"cell_type":"markdown","metadata":{"id":"WkrlfoilHpDY"},"source":["Adding fetoscope data"]},{"cell_type":"code","metadata":{"id":"Ff8p9VPNDDMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297681836,"user_tz":-60,"elapsed":22937,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"558b9110-9b30-4dc6-92af-141456c1fc21"},"source":["# Import PyDrive and associated libraries.\n","# This only needs to be done once per notebook.\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","\n","listed = drive.ListFile({'q': \"title contains 'anon001.zip'\"}).GetList()\n","for file in listed:\n","  print('title {}, id {}'.format(file['title'], file['id']))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["title anon001.zip, id 1flMv1QbSrxAH5IzV4xn5g9mljrZK-0rz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PJwtH20zHi0Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297691188,"user_tz":-60,"elapsed":3737,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"a76926fc-8c8f-40a6-b01b-e9bec4285f76"},"source":["# Import PyDrive and associated libraries.\n","# This only needs to be done once per notebook.\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import os\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","file_id = file['id']\n","\n","fname = os.path.join('/content/flownet2-pytorch', file['title'])\n","print('downloading to {}'.format(fname))\n","f_ = drive.CreateFile({'id': file_id})\n","f_.GetContentFile(fname)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["downloading to /content/flownet2-pytorch/anon001.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Da-yCZewIULP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613297708474,"user_tz":-60,"elapsed":1824,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"30cb7543-461d-4e41-bf6e-b7f188a729bf"},"source":["# unzip sample images\n","!unzip anon001.zip"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Archive:  anon001.zip\n","   creating: anon001/\n","  inflating: anon001/anon001_00831.png  \n","  inflating: anon001/anon001_00832.png  \n","  inflating: anon001/anon001_00833.png  \n","  inflating: anon001/anon001_00834.png  \n","  inflating: anon001/anon001_00835.png  \n","  inflating: anon001/anon001_00836.png  \n","  inflating: anon001/anon001_00837.png  \n","  inflating: anon001/anon001_00838.png  \n","  inflating: anon001/anon001_00839.png  \n","  inflating: anon001/anon001_00840.png  \n","  inflating: anon001/anon001_00841.png  \n","  inflating: anon001/anon001_00842.png  \n","  inflating: anon001/anon001_00843.png  \n","  inflating: anon001/anon001_00844.png  \n","  inflating: anon001/anon001_00845.png  \n","  inflating: anon001/anon001_00846.png  \n","  inflating: anon001/anon001_00847.png  \n","  inflating: anon001/anon001_00848.png  \n","  inflating: anon001/anon001_00849.png  \n","  inflating: anon001/anon001_00850.png  \n","  inflating: anon001/anon001_00851.png  \n","  inflating: anon001/anon001_00852.png  \n","  inflating: anon001/anon001_00853.png  \n","  inflating: anon001/anon001_00854.png  \n","  inflating: anon001/anon001_00855.png  \n","  inflating: anon001/anon001_00856.png  \n","  inflating: anon001/anon001_00857.png  \n","  inflating: anon001/anon001_00858.png  \n","  inflating: anon001/anon001_00859.png  \n","  inflating: anon001/anon001_00860.png  \n","  inflating: anon001/anon001_00861.png  \n","  inflating: anon001/anon001_00862.png  \n","  inflating: anon001/anon001_00863.png  \n","  inflating: anon001/anon001_00864.png  \n","  inflating: anon001/anon001_00865.png  \n","  inflating: anon001/anon001_00866.png  \n","  inflating: anon001/anon001_00867.png  \n","  inflating: anon001/anon001_00868.png  \n","  inflating: anon001/anon001_00869.png  \n","  inflating: anon001/anon001_00870.png  \n","  inflating: anon001/anon001_00871.png  \n","  inflating: anon001/anon001_00872.png  \n","  inflating: anon001/anon001_00873.png  \n","  inflating: anon001/anon001_00874.png  \n","  inflating: anon001/anon001_00875.png  \n","  inflating: anon001/anon001_00876.png  \n","  inflating: anon001/anon001_00877.png  \n","  inflating: anon001/anon001_00878.png  \n","  inflating: anon001/anon001_00879.png  \n","  inflating: anon001/anon001_00880.png  \n","  inflating: anon001/anon001_00881.png  \n","  inflating: anon001/anon001_00882.png  \n","  inflating: anon001/anon001_00883.png  \n","  inflating: anon001/anon001_00884.png  \n","  inflating: anon001/anon001_00885.png  \n","  inflating: anon001/anon001_00886.png  \n","  inflating: anon001/anon001_00887.png  \n","  inflating: anon001/anon001_00888.png  \n","  inflating: anon001/anon001_00889.png  \n","  inflating: anon001/anon001_00890.png  \n","  inflating: anon001/anon001_00891.png  \n","  inflating: anon001/anon001_00892.png  \n","  inflating: anon001/anon001_00893.png  \n","  inflating: anon001/anon001_00894.png  \n","  inflating: anon001/anon001_00895.png  \n","  inflating: anon001/anon001_00896.png  \n","  inflating: anon001/anon001_00897.png  \n","  inflating: anon001/anon001_00898.png  \n","  inflating: anon001/anon001_00899.png  \n","  inflating: anon001/anon001_00900.png  \n","  inflating: anon001/anon001_00901.png  \n","  inflating: anon001/anon001_00902.png  \n","  inflating: anon001/anon001_00903.png  \n","  inflating: anon001/anon001_00904.png  \n","  inflating: anon001/anon001_00905.png  \n","  inflating: anon001/anon001_00906.png  \n","  inflating: anon001/anon001_00907.png  \n","  inflating: anon001/anon001_00908.png  \n","  inflating: anon001/anon001_00909.png  \n","  inflating: anon001/anon001_00910.png  \n","  inflating: anon001/anon001_00911.png  \n","  inflating: anon001/anon001_00912.png  \n","  inflating: anon001/anon001_00913.png  \n","  inflating: anon001/anon001_00914.png  \n","  inflating: anon001/anon001_00915.png  \n","  inflating: anon001/anon001_00916.png  \n","  inflating: anon001/anon001_00917.png  \n","  inflating: anon001/anon001_00918.png  \n","  inflating: anon001/anon001_00919.png  \n","  inflating: anon001/anon001_00920.png  \n","  inflating: anon001/anon001_00921.png  \n","  inflating: anon001/anon001_00922.png  \n","  inflating: anon001/anon001_00923.png  \n","  inflating: anon001/anon001_00924.png  \n","  inflating: anon001/anon001_00925.png  \n","  inflating: anon001/anon001_00926.png  \n","  inflating: anon001/anon001_00927.png  \n","  inflating: anon001/anon001_00928.png  \n","  inflating: anon001/anon001_00929.png  \n","  inflating: anon001/anon001_00930.png  \n","  inflating: anon001/anon001_00931.png  \n","  inflating: anon001/anon001_00932.png  \n","  inflating: anon001/anon001_00933.png  \n","  inflating: anon001/anon001_00934.png  \n","  inflating: anon001/anon001_00935.png  \n","  inflating: anon001/anon001_00936.png  \n","  inflating: anon001/anon001_00937.png  \n","  inflating: anon001/anon001_00938.png  \n","  inflating: anon001/anon001_00939.png  \n","  inflating: anon001/anon001_00940.png  \n","  inflating: anon001/anon001_00941.png  \n","  inflating: anon001/anon001_00942.png  \n","  inflating: anon001/anon001_00943.png  \n","  inflating: anon001/anon001_00944.png  \n","  inflating: anon001/anon001_00945.png  \n","  inflating: anon001/anon001_00946.png  \n","  inflating: anon001/anon001_00947.png  \n","  inflating: anon001/anon001_00948.png  \n","  inflating: anon001/anon001_00949.png  \n","  inflating: anon001/anon001_00950.png  \n","  inflating: anon001/anon001_00951.png  \n","  inflating: anon001/anon001_00952.png  \n","  inflating: anon001/anon001_00953.png  \n","  inflating: anon001/anon001_00954.png  \n","  inflating: anon001/anon001_00955.png  \n","  inflating: anon001/anon001_00956.png  \n","  inflating: anon001/anon001_00957.png  \n","  inflating: anon001/anon001_00958.png  \n","  inflating: anon001/anon001_00959.png  \n","  inflating: anon001/anon001_00960.png  \n","  inflating: anon001/anon001_00961.png  \n","  inflating: anon001/anon001_00962.png  \n","  inflating: anon001/anon001_00963.png  \n","  inflating: anon001/anon001_00964.png  \n","  inflating: anon001/anon001_00965.png  \n","  inflating: anon001/anon001_00966.png  \n","  inflating: anon001/anon001_00967.png  \n","  inflating: anon001/anon001_00968.png  \n","  inflating: anon001/anon001_00969.png  \n","  inflating: anon001/anon001_00970.png  \n","  inflating: anon001/anon001_00971.png  \n","  inflating: anon001/anon001_00972.png  \n","  inflating: anon001/anon001_00973.png  \n","  inflating: anon001/anon001_00974.png  \n","  inflating: anon001/anon001_00975.png  \n","  inflating: anon001/anon001_00976.png  \n","  inflating: anon001/anon001_00977.png  \n","  inflating: anon001/anon001_00978.png  \n","  inflating: anon001/anon001_00979.png  \n","  inflating: anon001/anon001_00980.png  \n","  inflating: anon001/anon001_00981.png  \n","  inflating: anon001/anon001_00982.png  \n","  inflating: anon001/anon001_00983.png  \n","  inflating: anon001/anon001_00984.png  \n","  inflating: anon001/anon001_00985.png  \n","  inflating: anon001/anon001_00986.png  \n","  inflating: anon001/anon001_00987.png  \n","  inflating: anon001/anon001_00988.png  \n","  inflating: anon001/anon001_00989.png  \n","  inflating: anon001/anon001_00990.png  \n","  inflating: anon001/anon001_00991.png  \n","  inflating: anon001/anon001_00992.png  \n","  inflating: anon001/anon001_00993.png  \n","  inflating: anon001/anon001_00994.png  \n","  inflating: anon001/anon001_00995.png  \n","  inflating: anon001/anon001_00996.png  \n","  inflating: anon001/anon001_00997.png  \n","  inflating: anon001/anon001_00998.png  \n","  inflating: anon001/anon001_00999.png  \n","  inflating: anon001/anon001_01000.png  \n","  inflating: anon001/anon001_01001.png  \n","  inflating: anon001/anon001_01002.png  \n","  inflating: anon001/anon001_01003.png  \n","  inflating: anon001/anon001_01004.png  \n","  inflating: anon001/anon001_01005.png  \n","  inflating: anon001/anon001_01006.png  \n","  inflating: anon001/anon001_01007.png  \n","  inflating: anon001/anon001_01008.png  \n","  inflating: anon001/anon001_01009.png  \n","  inflating: anon001/anon001_01010.png  \n","  inflating: anon001/anon001_01011.png  \n","  inflating: anon001/anon001_01012.png  \n","  inflating: anon001/anon001_01013.png  \n","  inflating: anon001/anon001_01014.png  \n","  inflating: anon001/anon001_01015.png  \n","  inflating: anon001/anon001_01016.png  \n","  inflating: anon001/anon001_01017.png  \n","  inflating: anon001/anon001_01018.png  \n","  inflating: anon001/anon001_01019.png  \n","  inflating: anon001/anon001_01020.png  \n","  inflating: anon001/anon001_01021.png  \n","  inflating: anon001/anon001_01022.png  \n","  inflating: anon001/anon001_01023.png  \n","  inflating: anon001/anon001_01024.png  \n","  inflating: anon001/anon001_01025.png  \n","  inflating: anon001/anon001_01026.png  \n","  inflating: anon001/anon001_01027.png  \n","  inflating: anon001/anon001_01028.png  \n","  inflating: anon001/anon001_01029.png  \n","  inflating: anon001/anon001_01030.png  \n","  inflating: anon001/anon001_01031.png  \n","  inflating: anon001/anon001_01032.png  \n","  inflating: anon001/anon001_01033.png  \n","  inflating: anon001/anon001_01034.png  \n","  inflating: anon001/anon001_01035.png  \n","  inflating: anon001/anon001_01036.png  \n","  inflating: anon001/anon001_01037.png  \n","  inflating: anon001/anon001_01038.png  \n","  inflating: anon001/anon001_01039.png  \n","  inflating: anon001/anon001_01040.png  \n","  inflating: anon001/anon001_01041.png  \n","  inflating: anon001/anon001_01042.png  \n","  inflating: anon001/anon001_01043.png  \n","  inflating: anon001/anon001_01044.png  \n","  inflating: anon001/anon001_01045.png  \n","  inflating: anon001/anon001_01046.png  \n","  inflating: anon001/anon001_01047.png  \n","  inflating: anon001/anon001_01048.png  \n","  inflating: anon001/anon001_01049.png  \n","  inflating: anon001/anon001_01050.png  \n","  inflating: anon001/anon001_01051.png  \n","  inflating: anon001/anon001_01052.png  \n","  inflating: anon001/anon001_01053.png  \n","  inflating: anon001/anon001_01054.png  \n","  inflating: anon001/anon001_01055.png  \n","  inflating: anon001/anon001_01056.png  \n","  inflating: anon001/anon001_01057.png  \n","  inflating: anon001/anon001_01058.png  \n","  inflating: anon001/anon001_01059.png  \n","  inflating: anon001/anon001_01060.png  \n","  inflating: anon001/anon001_01061.png  \n","  inflating: anon001/anon001_01062.png  \n","  inflating: anon001/anon001_01063.png  \n","  inflating: anon001/anon001_01064.png  \n","  inflating: anon001/anon001_01065.png  \n","  inflating: anon001/anon001_01066.png  \n","  inflating: anon001/anon001_01067.png  \n","  inflating: anon001/anon001_01068.png  \n","  inflating: anon001/anon001_01069.png  \n","  inflating: anon001/anon001_01070.png  \n","  inflating: anon001/anon001_01071.png  \n","  inflating: anon001/anon001_01072.png  \n","  inflating: anon001/anon001_01073.png  \n","  inflating: anon001/anon001_01074.png  \n","  inflating: anon001/anon001_01075.png  \n","  inflating: anon001/anon001_01076.png  \n","  inflating: anon001/anon001_01077.png  \n","  inflating: anon001/anon001_01078.png  \n","  inflating: anon001/anon001_01079.png  \n","  inflating: anon001/anon001_01080.png  \n","  inflating: anon001/anon001_01081.png  \n","  inflating: anon001/anon001_01082.png  \n","  inflating: anon001/anon001_01083.png  \n","  inflating: anon001/anon001_01084.png  \n","  inflating: anon001/anon001_01085.png  \n","  inflating: anon001/anon001_01086.png  \n","  inflating: anon001/anon001_01087.png  \n","  inflating: anon001/anon001_01088.png  \n","  inflating: anon001/anon001_01089.png  \n","  inflating: anon001/anon001_01090.png  \n","  inflating: anon001/anon001_01091.png  \n","  inflating: anon001/anon001_01092.png  \n","  inflating: anon001/anon001_01093.png  \n","  inflating: anon001/anon001_01094.png  \n","  inflating: anon001/anon001_01095.png  \n","  inflating: anon001/anon001_01096.png  \n","  inflating: anon001/anon001_01097.png  \n","  inflating: anon001/anon001_01098.png  \n","  inflating: anon001/anon001_01099.png  \n","  inflating: anon001/anon001_01100.png  \n","  inflating: anon001/anon001_01101.png  \n","  inflating: anon001/anon001_01102.png  \n","  inflating: anon001/anon001_01103.png  \n","  inflating: anon001/anon001_01104.png  \n","  inflating: anon001/anon001_01105.png  \n","  inflating: anon001/anon001_01106.png  \n","  inflating: anon001/anon001_01107.png  \n","  inflating: anon001/anon001_01108.png  \n","  inflating: anon001/anon001_01109.png  \n","  inflating: anon001/anon001_01110.png  \n","  inflating: anon001/anon001_01111.png  \n","  inflating: anon001/anon001_01112.png  \n","  inflating: anon001/anon001_01113.png  \n","  inflating: anon001/anon001_01114.png  \n","  inflating: anon001/anon001_01115.png  \n","  inflating: anon001/anon001_01116.png  \n","  inflating: anon001/anon001_01117.png  \n","  inflating: anon001/anon001_01118.png  \n","  inflating: anon001/anon001_01119.png  \n","  inflating: anon001/anon001_01120.png  \n","  inflating: anon001/anon001_01121.png  \n","  inflating: anon001/anon001_01122.png  \n","  inflating: anon001/anon001_01123.png  \n","  inflating: anon001/anon001_01124.png  \n","  inflating: anon001/anon001_01125.png  \n","  inflating: anon001/anon001_01126.png  \n","  inflating: anon001/anon001_01127.png  \n","  inflating: anon001/anon001_01128.png  \n","  inflating: anon001/anon001_01129.png  \n","  inflating: anon001/anon001_01130.png  \n","  inflating: anon001/anon001_01131.png  \n","  inflating: anon001/anon001_01132.png  \n","  inflating: anon001/anon001_01133.png  \n","  inflating: anon001/anon001_01134.png  \n","  inflating: anon001/anon001_01135.png  \n","  inflating: anon001/anon001_01136.png  \n","  inflating: anon001/anon001_01137.png  \n","  inflating: anon001/anon001_01138.png  \n","  inflating: anon001/anon001_01139.png  \n","  inflating: anon001/anon001_01140.png  \n","  inflating: anon001/anon001_01141.png  \n","  inflating: anon001/anon001_01142.png  \n","  inflating: anon001/anon001_01143.png  \n","  inflating: anon001/anon001_01144.png  \n","  inflating: anon001/anon001_01145.png  \n","  inflating: anon001/anon001_01146.png  \n","  inflating: anon001/anon001_01147.png  \n","  inflating: anon001/anon001_01148.png  \n","  inflating: anon001/anon001_01149.png  \n","  inflating: anon001/anon001_01150.png  \n","  inflating: anon001/anon001_01151.png  \n","  inflating: anon001/anon001_01152.png  \n","  inflating: anon001/anon001_01153.png  \n","  inflating: anon001/anon001_01154.png  \n","  inflating: anon001/anon001_01155.png  \n","  inflating: anon001/anon001_01156.png  \n","  inflating: anon001/anon001_01157.png  \n","  inflating: anon001/anon001_01158.png  \n","  inflating: anon001/anon001_01159.png  \n","  inflating: anon001/anon001_01160.png  \n","  inflating: anon001/anon001_01161.png  \n","  inflating: anon001/anon001_01162.png  \n","  inflating: anon001/anon001_01163.png  \n","  inflating: anon001/anon001_01164.png  \n","  inflating: anon001/anon001_01165.png  \n","  inflating: anon001/anon001_01166.png  \n","  inflating: anon001/anon001_01167.png  \n","  inflating: anon001/anon001_01168.png  \n","  inflating: anon001/anon001_01169.png  \n","  inflating: anon001/anon001_01170.png  \n","  inflating: anon001/anon001_01171.png  \n","  inflating: anon001/anon001_01172.png  \n","  inflating: anon001/anon001_01173.png  \n","  inflating: anon001/anon001_01174.png  \n","  inflating: anon001/anon001_01175.png  \n","  inflating: anon001/anon001_01176.png  \n","  inflating: anon001/anon001_01177.png  \n","  inflating: anon001/anon001_01178.png  \n","  inflating: anon001/anon001_01179.png  \n","  inflating: anon001/anon001_01180.png  \n","  inflating: anon001/anon001_01181.png  \n","  inflating: anon001/anon001_01182.png  \n","  inflating: anon001/anon001_01183.png  \n","  inflating: anon001/anon001_01184.png  \n","  inflating: anon001/anon001_01185.png  \n","  inflating: anon001/anon001_01186.png  \n","  inflating: anon001/anon001_01187.png  \n","  inflating: anon001/anon001_01188.png  \n","  inflating: anon001/anon001_01189.png  \n","  inflating: anon001/anon001_01190.png  \n","  inflating: anon001/anon001_01191.png  \n","  inflating: anon001/anon001_01192.png  \n","  inflating: anon001/anon001_01193.png  \n","  inflating: anon001/anon001_01194.png  \n","  inflating: anon001/anon001_01195.png  \n","  inflating: anon001/anon001_01196.png  \n","  inflating: anon001/anon001_01197.png  \n","  inflating: anon001/anon001_01198.png  \n","  inflating: anon001/anon001_01199.png  \n","  inflating: anon001/anon001_01200.png  \n","  inflating: anon001/anon001_01201.png  \n","  inflating: anon001/anon001_01202.png  \n","  inflating: anon001/anon001_01203.png  \n","  inflating: anon001/anon001_01204.png  \n","  inflating: anon001/anon001_01205.png  \n","  inflating: anon001/anon001_01206.png  \n","  inflating: anon001/anon001_01207.png  \n","  inflating: anon001/anon001_01208.png  \n","  inflating: anon001/anon001_01209.png  \n","  inflating: anon001/anon001_01210.png  \n","  inflating: anon001/anon001_01211.png  \n","  inflating: anon001/anon001_01212.png  \n","  inflating: anon001/anon001_01213.png  \n","  inflating: anon001/anon001_01214.png  \n","  inflating: anon001/anon001_01215.png  \n","  inflating: anon001/anon001_01216.png  \n","  inflating: anon001/anon001_01217.png  \n","  inflating: anon001/anon001_01218.png  \n","  inflating: anon001/anon001_01219.png  \n","  inflating: anon001/anon001_01220.png  \n","  inflating: anon001/anon001_01221.png  \n","  inflating: anon001/anon001_01222.png  \n","  inflating: anon001/anon001_01223.png  \n","  inflating: anon001/anon001_01224.png  \n","  inflating: anon001/anon001_01225.png  \n","  inflating: anon001/anon001_01226.png  \n","  inflating: anon001/anon001_01227.png  \n","  inflating: anon001/anon001_01228.png  \n","  inflating: anon001/anon001_01229.png  \n","  inflating: anon001/anon001_01230.png  \n","  inflating: anon001/anon001_01231.png  \n","  inflating: anon001/anon001_01232.png  \n","  inflating: anon001/anon001_01233.png  \n","  inflating: anon001/anon001_01234.png  \n","  inflating: anon001/anon001_01235.png  \n","  inflating: anon001/anon001_01236.png  \n","  inflating: anon001/anon001_01237.png  \n","  inflating: anon001/anon001_01238.png  \n","  inflating: anon001/anon001_01239.png  \n","  inflating: anon001/anon001_01240.png  \n","  inflating: anon001/anon001_01241.png  \n","  inflating: anon001/anon001_01242.png  \n","  inflating: anon001/anon001_01243.png  \n","  inflating: anon001/anon001_01244.png  \n","  inflating: anon001/anon001_01245.png  \n","  inflating: anon001/anon001_01246.png  \n","  inflating: anon001/anon001_01247.png  \n","  inflating: anon001/anon001_01248.png  \n","  inflating: anon001/anon001_01249.png  \n","  inflating: anon001/anon001_01250.png  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8UdFvHWsvtBr"},"source":["**resize images to 448x448**"]},{"cell_type":"code","metadata":{"id":"JwP47UoZ8Rug","executionInfo":{"status":"ok","timestamp":1613297712790,"user_tz":-60,"elapsed":505,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}}},"source":["from PIL import Image\r\n","import numpy as np"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXTUMEkT2z8W","executionInfo":{"status":"ok","timestamp":1613297725648,"user_tz":-60,"elapsed":482,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}}},"source":["IMG_SHAPE_STANDARD = (448,448);\r\n","frames_path = \"/content/flownet2-pytorch/anon001\""],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UGU_idgvrrE","executionInfo":{"status":"ok","timestamp":1613297769758,"user_tz":-60,"elapsed":42813,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}}},"source":["def resize_all_images(path):\r\n","  dirs = os.listdir(path)\r\n","  for item in dirs:\r\n","      if item.lower().endswith(('.png', '.jpg', '.jpeg')):\r\n","        im = Image.open(os.path.join(path,item))\r\n","        # f, e = os.path.splitext(path+item)\r\n","        imResize = im.resize(IMG_SHAPE_STANDARD, Image.ANTIALIAS)\r\n","        imResize.save(os.path.join(path,item), 'PNG')\r\n","\r\n","resize_all_images(frames_path)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dplycGstVrVM"},"source":["Note: the pretrained flownet models can be gotten from the github account. Just clicking on them links them to your gdrive, or one can save it and use it later"]},{"cell_type":"code","metadata":{"id":"rAirEAK8I7kp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613298828360,"user_tz":-60,"elapsed":1059,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"37f6de7d-85d9-49b2-bf86-486edc7a84f6"},"source":["# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# List .txt files in the root.\n","#\n","# Search query reference:\n","# https://developers.google.com/drive/v2/web/search-parameters\n","listed = drive.ListFile({'q': \"title contains 'FlowNet2_checkpoint.pth.tar'\"}).GetList()\n","for file in listed:\n","  print('title {}, id {}'.format(file['title'], file['id']))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["title FlowNet2_checkpoint.pth.tar, id 1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KY6UZDvz_fxg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613298890197,"user_tz":-60,"elapsed":59957,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"4ef08b2e-bbea-4cd2-b076-90fdd6afd0d5"},"source":["# Import PyDrive and associated libraries.\n","# This only needs to be done once per notebook.\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import os\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","file_id = '1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da' \n","\n","fname = os.path.join('/content/flownet2-pytorch/pretrained', 'FlowNet2_checkpoint.pth.tar')\n","print('downloading to {}'.format(fname))\n","f_ = drive.CreateFile({'id': file_id})\n","f_.GetContentFile(fname)\n","\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["downloading to /content/flownet2-pytorch/pretrained/FlowNet2_checkpoint.pth.tar\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nu-_SrLlkHtI"},"source":["Perform evaluation with the pretrained models"]},{"cell_type":"code","metadata":{"id":"_--9MsdTJM1k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613298942742,"user_tz":-60,"elapsed":52188,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"f773e9c6-7e01-400f-dd43-ed55bd967d05"},"source":["## Run our own data\n","!python main.py --inference --model FlowNet2 --save_flow \\\n","--inference_dataset ImagesFromFolder \\\n","--inference_dataset_root /content/flownet2-pytorch/anon001 \\\n","--resume /content/flownet2-pytorch/pretrained/FlowNet2_checkpoint.pth.tar \\\n","--save /content/flownet2-pytorch/results"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Parsing Arguments\n","  [0.030s] \u001b[0mbatch_size: 8\u001b[0m\n","  [0.030s] \u001b[0mcrop_size: [256, 256]\u001b[0m\n","  [0.030s] \u001b[0mfp16: False\u001b[0m\n","  [0.030s] \u001b[0mfp16_scale: 1024.0\u001b[0m\n","  [0.030s] \u001b[0mgradient_clip: None\u001b[0m\n","  [0.030s] \u001b[35minference: True\u001b[0m\n","  [0.030s] \u001b[0minference_batch_size: 1\u001b[0m\n","  [0.030s] \u001b[35minference_dataset: ImagesFromFolder\u001b[0m\n","  [0.030s] \u001b[0minference_dataset_iext: png\u001b[0m\n","  [0.030s] \u001b[0minference_dataset_replicates: 1\u001b[0m\n","  [0.030s] \u001b[35minference_dataset_root: /content/flownet2-pytorch/anon001\u001b[0m\n","  [0.030s] \u001b[0minference_n_batches: -1\u001b[0m\n","  [0.030s] \u001b[0minference_size: [-1, -1]\u001b[0m\n","  [0.030s] \u001b[0minference_visualize: False\u001b[0m\n","  [0.030s] \u001b[0mlog_frequency: 1\u001b[0m\n","  [0.030s] \u001b[0mloss: L1Loss\u001b[0m\n","  [0.030s] \u001b[0mmodel: FlowNet2\u001b[0m\n","  [0.030s] \u001b[0mmodel_batchNorm: False\u001b[0m\n","  [0.030s] \u001b[0mmodel_div_flow: 20.0\u001b[0m\n","  [0.030s] \u001b[0mname: run\u001b[0m\n","  [0.030s] \u001b[0mno_cuda: False\u001b[0m\n","  [0.030s] \u001b[35mnumber_gpus: 1\u001b[0m\n","  [0.030s] \u001b[0mnumber_workers: 8\u001b[0m\n","  [0.030s] \u001b[0moptimizer: Adam\u001b[0m\n","  [0.031s] \u001b[0moptimizer_amsgrad: False\u001b[0m\n","  [0.031s] \u001b[0moptimizer_betas: (0.9, 0.999)\u001b[0m\n","  [0.031s] \u001b[0moptimizer_eps: 1e-08\u001b[0m\n","  [0.031s] \u001b[0moptimizer_lr: 0.001\u001b[0m\n","  [0.031s] \u001b[0moptimizer_weight_decay: 0\u001b[0m\n","  [0.031s] \u001b[0mrender_validation: False\u001b[0m\n","  [0.031s] \u001b[35mresume: /content/flownet2-pytorch/pretrained/FlowNet2_checkpoint.pth.tar\u001b[0m\n","  [0.031s] \u001b[0mrgb_max: 255.0\u001b[0m\n","  [0.031s] \u001b[35msave: /content/flownet2-pytorch/results\u001b[0m\n","  [0.031s] \u001b[35msave_flow: True\u001b[0m\n","  [0.031s] \u001b[0mschedule_lr_fraction: 10\u001b[0m\n","  [0.031s] \u001b[0mschedule_lr_frequency: 0\u001b[0m\n","  [0.031s] \u001b[0mseed: 1\u001b[0m\n","  [0.031s] \u001b[0mskip_training: False\u001b[0m\n","  [0.031s] \u001b[0mskip_validation: False\u001b[0m\n","  [0.031s] \u001b[0mstart_epoch: 1\u001b[0m\n","  [0.031s] \u001b[0mtotal_epochs: 10000\u001b[0m\n","  [0.031s] \u001b[0mtrain_n_batches: -1\u001b[0m\n","  [0.031s] \u001b[0mtraining_dataset: MpiSintelFinal\u001b[0m\n","  [0.031s] \u001b[0mtraining_dataset_replicates: 1\u001b[0m\n","  [0.031s] \u001b[0mtraining_dataset_root: ./MPI-Sintel/flow/training\u001b[0m\n","  [0.031s] \u001b[0mvalidation_dataset: MpiSintelClean\u001b[0m\n","  [0.031s] \u001b[0mvalidation_dataset_replicates: 1\u001b[0m\n","  [0.031s] \u001b[0mvalidation_dataset_root: ./MPI-Sintel/flow/training\u001b[0m\n","  [0.031s] \u001b[0mvalidation_frequency: 5\u001b[0m\n","  [0.031s] \u001b[0mvalidation_n_batches: -1\u001b[0m\n","  [0.033s] Operation finished\n","\n","Source Code\n","  Current Git Hash: b'2e9e010c98931bc7cef3eb063b195f1e0ab470ba'\n","\n","Initializing Datasets\n","  [0.017s] Inference Dataset: ImagesFromFolder\n","  [0.037s] Inference Input: [3, 2, 448, 448]\n","  [0.053s] Inference Targets: [3, 2, 448, 448]\n","  [0.053s] Operation finished\n","\n","Building FlowNet2 model\n","  [3.023s] Effective Batch Size: 8\n","  [3.025s] Number of parameters: 162518834\n","  [3.025s] Initializing CUDA\n","  [6.175s] Parallelizing\n","  [6.180s] Loading checkpoint '/content/flownet2-pytorch/pretrained/FlowNet2_checkpoint.pth.tar'\n","  [6.551s] Loaded checkpoint '/content/flownet2-pytorch/pretrained/FlowNet2_checkpoint.pth.tar' (at epoch 0)\n","  [6.551s] Initializing save directory: /content/flownet2-pytorch/results\n","  [6.555s] Operation finished\n","\n","Initializing Adam Optimizer\n","  [0.002s] amsgrad = False (<class 'bool'>)\n","  [0.002s] weight_decay = 0 (<class 'int'>)\n","  [0.002s] eps = 1e-08 (<class 'float'>)\n","  [0.002s] betas = (0.9, 0.999) (<class 'tuple'>)\n","  [0.002s] lr = 0.001 (<class 'float'>)\n","  [0.002s] Operation finished\n","\n","Overall Progress:   0%|                                                       | 0/1 [00:00<?, ?it/s]\n","Inferencing :   0%|                                                       | 0/419.0 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","\n","Inference Averages for Epoch 0: L1: 7.521, EPE: 13.026:   0%|             | 0/419.0 [00:00<?, ?it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 7.521, EPE: 13.026:   0%|     | 1/419.0 [00:00<04:30,  1.54it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 7.521, EPE: 13.026:   0%|     | 1/419.0 [00:00<04:31,  1.54it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.777, EPE: 11.739:   0%|     | 1/419.0 [00:00<04:31,  1.54it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.777, EPE: 11.739:   0%|     | 2/419.0 [00:00<03:22,  2.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.777, EPE: 11.739:   0%|     | 2/419.0 [00:00<03:22,  2.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.543, EPE: 11.332:   0%|     | 2/419.0 [00:00<03:22,  2.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.543, EPE: 11.332:   1%|     | 3/419.0 [00:00<02:33,  2.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.543, EPE: 11.332:   1%|     | 3/419.0 [00:00<02:33,  2.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.273, EPE: 10.865:   1%|     | 3/419.0 [00:00<02:33,  2.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.352, EPE: 11.001:   1%|     | 4/419.0 [00:01<02:33,  2.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.352, EPE: 11.001:   1%|     | 5/419.0 [00:01<01:57,  3.52it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.352, EPE: 11.001:   1%|     | 5/419.0 [00:01<01:57,  3.52it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.222, EPE: 10.776:   1%|     | 5/419.0 [00:01<01:57,  3.52it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.084, EPE: 10.538:   1%|     | 6/419.0 [00:01<01:57,  3.52it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.084, EPE: 10.538:   2%|     | 7/419.0 [00:01<01:31,  4.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 6.084, EPE: 10.538:   2%|     | 7/419.0 [00:01<01:31,  4.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.935, EPE: 10.279:   2%|     | 7/419.0 [00:01<01:31,  4.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.821, EPE: 10.083:   2%|     | 8/419.0 [00:01<01:31,  4.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.821, EPE: 10.083:   2%|     | 9/419.0 [00:01<01:14,  5.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.821, EPE: 10.083:   2%|     | 9/419.0 [00:01<01:14,  5.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.725, EPE: 9.916:   2%|▏     | 9/419.0 [00:01<01:14,  5.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.636, EPE: 9.762:   2%|     | 10/419.0 [00:01<01:13,  5.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.636, EPE: 9.762:   3%|▏    | 11/419.0 [00:01<01:01,  6.61it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.636, EPE: 9.762:   3%|▏    | 11/419.0 [00:01<01:01,  6.61it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.578, EPE: 9.662:   3%|▏    | 11/419.0 [00:01<01:01,  6.61it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.543, EPE: 9.600:   3%|▏    | 12/419.0 [00:01<01:01,  6.61it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.543, EPE: 9.600:   3%|▏    | 13/419.0 [00:01<00:52,  7.67it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.543, EPE: 9.600:   3%|▏    | 13/419.0 [00:01<00:52,  7.66it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.461, EPE: 9.459:   3%|▏    | 13/419.0 [00:01<00:52,  7.66it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.387, EPE: 9.331:   3%|▏    | 14/419.0 [00:01<00:52,  7.66it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.387, EPE: 9.331:   4%|▏    | 15/419.0 [00:01<00:46,  8.60it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.387, EPE: 9.331:   4%|▏    | 15/419.0 [00:01<00:47,  8.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.268, EPE: 9.124:   4%|▏    | 15/419.0 [00:01<00:47,  8.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.142, EPE: 8.907:   4%|▏    | 16/419.0 [00:02<00:46,  8.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.142, EPE: 8.907:   4%|▏    | 17/419.0 [00:02<00:43,  9.29it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.142, EPE: 8.907:   4%|▏    | 17/419.0 [00:02<00:43,  9.29it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 5.018, EPE: 8.691:   4%|▏    | 17/419.0 [00:02<00:43,  9.29it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.916, EPE: 8.515:   4%|▏    | 18/419.0 [00:02<00:43,  9.29it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.916, EPE: 8.515:   5%|▏    | 19/419.0 [00:02<00:40,  9.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.916, EPE: 8.515:   5%|▏    | 19/419.0 [00:02<00:40,  9.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.848, EPE: 8.397:   5%|▏    | 19/419.0 [00:02<00:40,  9.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.785, EPE: 8.287:   5%|▏    | 20/419.0 [00:02<00:39,  9.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.785, EPE: 8.287:   5%|▎    | 21/419.0 [00:02<00:37, 10.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.785, EPE: 8.287:   5%|▎    | 21/419.0 [00:02<00:37, 10.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.740, EPE: 8.210:   5%|▎    | 21/419.0 [00:02<00:37, 10.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.673, EPE: 8.094:   5%|▎    | 22/419.0 [00:02<00:37, 10.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.673, EPE: 8.094:   5%|▎    | 23/419.0 [00:02<00:35, 11.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.673, EPE: 8.094:   5%|▎    | 23/419.0 [00:02<00:35, 11.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.627, EPE: 8.014:   5%|▎    | 23/419.0 [00:02<00:35, 11.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.583, EPE: 7.938:   6%|▎    | 24/419.0 [00:02<00:35, 11.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.583, EPE: 7.938:   6%|▎    | 25/419.0 [00:02<00:34, 11.39it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.583, EPE: 7.938:   6%|▎    | 25/419.0 [00:02<00:34, 11.38it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.535, EPE: 7.855:   6%|▎    | 25/419.0 [00:02<00:34, 11.38it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.489, EPE: 7.775:   6%|▎    | 26/419.0 [00:02<00:34, 11.38it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.489, EPE: 7.775:   6%|▎    | 27/419.0 [00:02<00:33, 11.63it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.489, EPE: 7.775:   6%|▎    | 27/419.0 [00:02<00:33, 11.63it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.441, EPE: 7.693:   6%|▎    | 27/419.0 [00:02<00:33, 11.63it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.392, EPE: 7.607:   7%|▎    | 28/419.0 [00:02<00:33, 11.63it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.392, EPE: 7.607:   7%|▎    | 29/419.0 [00:02<00:32, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.392, EPE: 7.607:   7%|▎    | 29/419.0 [00:02<00:32, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.336, EPE: 7.510:   7%|▎    | 29/419.0 [00:03<00:32, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.267, EPE: 7.390:   7%|▎    | 30/419.0 [00:03<00:32, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.267, EPE: 7.390:   7%|▎    | 31/419.0 [00:03<00:32, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.267, EPE: 7.390:   7%|▎    | 31/419.0 [00:03<00:32, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.201, EPE: 7.276:   7%|▎    | 31/419.0 [00:03<00:32, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.153, EPE: 7.193:   8%|▍    | 32/419.0 [00:03<00:32, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.153, EPE: 7.193:   8%|▍    | 33/419.0 [00:03<00:32, 11.96it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.153, EPE: 7.193:   8%|▍    | 33/419.0 [00:03<00:32, 11.96it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.130, EPE: 7.153:   8%|▍    | 33/419.0 [00:03<00:32, 11.96it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.074, EPE: 7.057:   8%|▍    | 34/419.0 [00:03<00:32, 11.96it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.074, EPE: 7.057:   8%|▍    | 35/419.0 [00:03<00:32, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.074, EPE: 7.057:   8%|▍    | 35/419.0 [00:03<00:32, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.046, EPE: 7.008:   8%|▍    | 35/419.0 [00:03<00:32, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.033, EPE: 6.985:   9%|▍    | 36/419.0 [00:03<00:31, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.033, EPE: 6.985:   9%|▍    | 37/419.0 [00:03<00:31, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.033, EPE: 6.985:   9%|▍    | 37/419.0 [00:03<00:31, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.039, EPE: 6.996:   9%|▍    | 37/419.0 [00:03<00:31, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.036, EPE: 6.990:   9%|▍    | 38/419.0 [00:03<00:31, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.036, EPE: 6.990:   9%|▍    | 39/419.0 [00:03<00:31, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.036, EPE: 6.990:   9%|▍    | 39/419.0 [00:03<00:31, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.024, EPE: 6.970:   9%|▍    | 39/419.0 [00:03<00:31, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.065, EPE: 7.040:  10%|▍    | 40/419.0 [00:03<00:31, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.065, EPE: 7.040:  10%|▍    | 41/419.0 [00:03<00:31, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.065, EPE: 7.040:  10%|▍    | 41/419.0 [00:03<00:31, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.036, EPE: 6.991:  10%|▍    | 41/419.0 [00:04<00:31, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.016, EPE: 6.957:  10%|▌    | 42/419.0 [00:04<00:31, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.016, EPE: 6.957:  10%|▌    | 43/419.0 [00:04<00:31, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 4.016, EPE: 6.957:  10%|▌    | 43/419.0 [00:04<00:31, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.984, EPE: 6.900:  10%|▌    | 43/419.0 [00:04<00:31, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.961, EPE: 6.861:  11%|▌    | 44/419.0 [00:04<00:30, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.961, EPE: 6.861:  11%|▌    | 45/419.0 [00:04<00:30, 12.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.961, EPE: 6.861:  11%|▌    | 45/419.0 [00:04<00:30, 12.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.920, EPE: 6.789:  11%|▌    | 45/419.0 [00:04<00:30, 12.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.874, EPE: 6.711:  11%|▌    | 46/419.0 [00:04<00:30, 12.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.874, EPE: 6.711:  11%|▌    | 47/419.0 [00:04<00:30, 12.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.874, EPE: 6.711:  11%|▌    | 47/419.0 [00:04<00:30, 12.14it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.839, EPE: 6.649:  11%|▌    | 47/419.0 [00:04<00:30, 12.14it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.812, EPE: 6.602:  11%|▌    | 48/419.0 [00:04<00:30, 12.14it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.812, EPE: 6.602:  12%|▌    | 49/419.0 [00:04<00:31, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.812, EPE: 6.602:  12%|▌    | 49/419.0 [00:04<00:31, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.771, EPE: 6.531:  12%|▌    | 49/419.0 [00:04<00:31, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.733, EPE: 6.466:  12%|▌    | 50/419.0 [00:04<00:30, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.733, EPE: 6.466:  12%|▌    | 51/419.0 [00:04<00:30, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.733, EPE: 6.466:  12%|▌    | 51/419.0 [00:04<00:30, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.707, EPE: 6.421:  12%|▌    | 51/419.0 [00:04<00:30, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.683, EPE: 6.379:  12%|▌    | 52/419.0 [00:04<00:30, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.683, EPE: 6.379:  13%|▋    | 53/419.0 [00:04<00:30, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.683, EPE: 6.379:  13%|▋    | 53/419.0 [00:04<00:30, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.655, EPE: 6.330:  13%|▋    | 53/419.0 [00:05<00:30, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.646, EPE: 6.315:  13%|▋    | 54/419.0 [00:05<00:30, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.646, EPE: 6.315:  13%|▋    | 55/419.0 [00:05<00:30, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.646, EPE: 6.315:  13%|▋    | 55/419.0 [00:05<00:30, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.632, EPE: 6.290:  13%|▋    | 55/419.0 [00:05<00:30, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.611, EPE: 6.255:  13%|▋    | 56/419.0 [00:05<00:29, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.611, EPE: 6.255:  14%|▋    | 57/419.0 [00:05<00:29, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.611, EPE: 6.255:  14%|▋    | 57/419.0 [00:05<00:29, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.604, EPE: 6.242:  14%|▋    | 57/419.0 [00:05<00:29, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.587, EPE: 6.213:  14%|▋    | 58/419.0 [00:05<00:29, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.587, EPE: 6.213:  14%|▋    | 59/419.0 [00:05<00:30, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.587, EPE: 6.213:  14%|▋    | 59/419.0 [00:05<00:30, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.569, EPE: 6.181:  14%|▋    | 59/419.0 [00:05<00:30, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.551, EPE: 6.151:  14%|▋    | 60/419.0 [00:05<00:29, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.551, EPE: 6.151:  15%|▋    | 61/419.0 [00:05<00:29, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.551, EPE: 6.151:  15%|▋    | 61/419.0 [00:05<00:29, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.539, EPE: 6.130:  15%|▋    | 61/419.0 [00:05<00:29, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.551, EPE: 6.151:  15%|▋    | 62/419.0 [00:05<00:29, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.551, EPE: 6.151:  15%|▊    | 63/419.0 [00:05<00:30, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.551, EPE: 6.151:  15%|▊    | 63/419.0 [00:05<00:30, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.556, EPE: 6.159:  15%|▊    | 63/419.0 [00:05<00:30, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.567, EPE: 6.179:  15%|▊    | 64/419.0 [00:05<00:30, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.567, EPE: 6.179:  16%|▊    | 65/419.0 [00:05<00:30, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.567, EPE: 6.179:  16%|▊    | 65/419.0 [00:05<00:30, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.575, EPE: 6.191:  16%|▊    | 65/419.0 [00:06<00:30, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.561, EPE: 6.167:  16%|▊    | 66/419.0 [00:06<00:30, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.561, EPE: 6.167:  16%|▊    | 67/419.0 [00:06<00:29, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.561, EPE: 6.167:  16%|▊    | 67/419.0 [00:06<00:29, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.540, EPE: 6.132:  16%|▊    | 67/419.0 [00:06<00:29, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.525, EPE: 6.106:  16%|▊    | 68/419.0 [00:06<00:29, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.525, EPE: 6.106:  16%|▊    | 69/419.0 [00:06<00:29, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.525, EPE: 6.106:  16%|▊    | 69/419.0 [00:06<00:29, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.516, EPE: 6.090:  16%|▊    | 69/419.0 [00:06<00:29, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.498, EPE: 6.058:  17%|▊    | 70/419.0 [00:06<00:29, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.498, EPE: 6.058:  17%|▊    | 71/419.0 [00:06<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.498, EPE: 6.058:  17%|▊    | 71/419.0 [00:06<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.482, EPE: 6.030:  17%|▊    | 71/419.0 [00:06<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.472, EPE: 6.013:  17%|▊    | 72/419.0 [00:06<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.472, EPE: 6.013:  17%|▊    | 73/419.0 [00:06<00:29, 11.71it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.472, EPE: 6.013:  17%|▊    | 73/419.0 [00:06<00:29, 11.71it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.465, EPE: 6.001:  17%|▊    | 73/419.0 [00:06<00:29, 11.71it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.455, EPE: 5.985:  18%|▉    | 74/419.0 [00:06<00:29, 11.71it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.455, EPE: 5.985:  18%|▉    | 75/419.0 [00:06<00:29, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.455, EPE: 5.985:  18%|▉    | 75/419.0 [00:06<00:29, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.446, EPE: 5.969:  18%|▉    | 75/419.0 [00:06<00:29, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.957:  18%|▉    | 76/419.0 [00:07<00:29, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.957:  18%|▉    | 77/419.0 [00:07<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.957:  18%|▉    | 77/419.0 [00:07<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.426, EPE: 5.934:  18%|▉    | 77/419.0 [00:07<00:29, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.422, EPE: 5.927:  19%|▉    | 78/419.0 [00:07<00:28, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.422, EPE: 5.927:  19%|▉    | 79/419.0 [00:07<00:28, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.422, EPE: 5.927:  19%|▉    | 79/419.0 [00:07<00:28, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.412, EPE: 5.909:  19%|▉    | 79/419.0 [00:07<00:28, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.407, EPE: 5.902:  19%|▉    | 80/419.0 [00:07<00:28, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.407, EPE: 5.902:  19%|▉    | 81/419.0 [00:07<00:29, 11.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.407, EPE: 5.902:  19%|▉    | 81/419.0 [00:07<00:29, 11.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.402, EPE: 5.892:  19%|▉    | 81/419.0 [00:07<00:29, 11.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.396, EPE: 5.883:  20%|▉    | 82/419.0 [00:07<00:29, 11.53it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.396, EPE: 5.883:  20%|▉    | 83/419.0 [00:07<00:29, 11.58it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.396, EPE: 5.883:  20%|▉    | 83/419.0 [00:07<00:29, 11.58it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.389, EPE: 5.870:  20%|▉    | 83/419.0 [00:07<00:29, 11.58it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.383, EPE: 5.860:  20%|█    | 84/419.0 [00:07<00:28, 11.58it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.383, EPE: 5.860:  20%|█    | 85/419.0 [00:07<00:28, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.383, EPE: 5.860:  20%|█    | 85/419.0 [00:07<00:28, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.374, EPE: 5.844:  20%|█    | 85/419.0 [00:07<00:28, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.370, EPE: 5.836:  21%|█    | 86/419.0 [00:07<00:28, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.370, EPE: 5.836:  21%|█    | 87/419.0 [00:07<00:28, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.370, EPE: 5.836:  21%|█    | 87/419.0 [00:07<00:28, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.360, EPE: 5.820:  21%|█    | 87/419.0 [00:07<00:28, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.353, EPE: 5.807:  21%|█    | 88/419.0 [00:08<00:28, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.353, EPE: 5.807:  21%|█    | 89/419.0 [00:08<00:28, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.353, EPE: 5.807:  21%|█    | 89/419.0 [00:08<00:28, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.355, EPE: 5.810:  21%|█    | 89/419.0 [00:08<00:28, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.351, EPE: 5.804:  21%|█    | 90/419.0 [00:08<00:27, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.351, EPE: 5.804:  22%|█    | 91/419.0 [00:08<00:27, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.351, EPE: 5.804:  22%|█    | 91/419.0 [00:08<00:27, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.353, EPE: 5.807:  22%|█    | 91/419.0 [00:08<00:27, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.358, EPE: 5.817:  22%|█    | 92/419.0 [00:08<00:27, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.358, EPE: 5.817:  22%|█    | 93/419.0 [00:08<00:27, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.358, EPE: 5.817:  22%|█    | 93/419.0 [00:08<00:27, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.363, EPE: 5.825:  22%|█    | 93/419.0 [00:08<00:27, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.373, EPE: 5.842:  22%|█    | 94/419.0 [00:08<00:27, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.373, EPE: 5.842:  23%|█▏   | 95/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.373, EPE: 5.842:  23%|█▏   | 95/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.385, EPE: 5.864:  23%|█▏   | 95/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.405, EPE: 5.898:  23%|█▏   | 96/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.405, EPE: 5.898:  23%|█▏   | 97/419.0 [00:08<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.405, EPE: 5.898:  23%|█▏   | 97/419.0 [00:08<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.419, EPE: 5.922:  23%|█▏   | 97/419.0 [00:08<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.429, EPE: 5.939:  23%|█▏   | 98/419.0 [00:08<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.429, EPE: 5.939:  24%|█▏   | 99/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.429, EPE: 5.939:  24%|█▏   | 99/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.428, EPE: 5.938:  24%|█▏   | 99/419.0 [00:08<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.421, EPE: 5.925:  24%|▉   | 100/419.0 [00:09<00:26, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.421, EPE: 5.925:  24%|▉   | 101/419.0 [00:09<00:26, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.421, EPE: 5.925:  24%|▉   | 101/419.0 [00:09<00:26, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.408, EPE: 5.903:  24%|▉   | 101/419.0 [00:09<00:26, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.392, EPE: 5.875:  24%|▉   | 102/419.0 [00:09<00:26, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.392, EPE: 5.875:  25%|▉   | 103/419.0 [00:09<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.392, EPE: 5.875:  25%|▉   | 103/419.0 [00:09<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.375, EPE: 5.846:  25%|▉   | 103/419.0 [00:09<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.364, EPE: 5.827:  25%|▉   | 104/419.0 [00:09<00:26, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.364, EPE: 5.827:  25%|█   | 105/419.0 [00:09<00:26, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.364, EPE: 5.827:  25%|█   | 105/419.0 [00:09<00:26, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.360, EPE: 5.820:  25%|█   | 105/419.0 [00:09<00:26, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.364, EPE: 5.826:  25%|█   | 106/419.0 [00:09<00:25, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.364, EPE: 5.826:  26%|█   | 107/419.0 [00:09<00:25, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.364, EPE: 5.826:  26%|█   | 107/419.0 [00:09<00:25, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.368, EPE: 5.834:  26%|█   | 107/419.0 [00:09<00:25, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.381, EPE: 5.855:  26%|█   | 108/419.0 [00:09<00:25, 12.11it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.381, EPE: 5.855:  26%|█   | 109/419.0 [00:09<00:25, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.381, EPE: 5.855:  26%|█   | 109/419.0 [00:09<00:25, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.389, EPE: 5.871:  26%|█   | 109/419.0 [00:09<00:25, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.397, EPE: 5.883:  26%|█   | 110/419.0 [00:09<00:25, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.397, EPE: 5.883:  26%|█   | 111/419.0 [00:09<00:25, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.397, EPE: 5.883:  26%|█   | 111/419.0 [00:09<00:25, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.401, EPE: 5.890:  26%|█   | 111/419.0 [00:09<00:25, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.400, EPE: 5.888:  27%|█   | 112/419.0 [00:10<00:25, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.400, EPE: 5.888:  27%|█   | 113/419.0 [00:10<00:25, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.400, EPE: 5.888:  27%|█   | 113/419.0 [00:10<00:25, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.392, EPE: 5.875:  27%|█   | 113/419.0 [00:10<00:25, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.382, EPE: 5.858:  27%|█   | 114/419.0 [00:10<00:25, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.382, EPE: 5.858:  27%|█   | 115/419.0 [00:10<00:25, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.382, EPE: 5.858:  27%|█   | 115/419.0 [00:10<00:25, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.373, EPE: 5.843:  27%|█   | 115/419.0 [00:10<00:25, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.363, EPE: 5.825:  28%|█   | 116/419.0 [00:10<00:25, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.363, EPE: 5.825:  28%|█   | 117/419.0 [00:10<00:25, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.363, EPE: 5.825:  28%|█   | 117/419.0 [00:10<00:25, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.358, EPE: 5.816:  28%|█   | 117/419.0 [00:10<00:25, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.355, EPE: 5.811:  28%|█▏  | 118/419.0 [00:10<00:24, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.355, EPE: 5.811:  28%|█▏  | 119/419.0 [00:10<00:24, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.355, EPE: 5.811:  28%|█▏  | 119/419.0 [00:10<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.347, EPE: 5.796:  28%|█▏  | 119/419.0 [00:10<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.346, EPE: 5.796:  29%|█▏  | 120/419.0 [00:10<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.346, EPE: 5.796:  29%|█▏  | 121/419.0 [00:10<00:24, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.346, EPE: 5.796:  29%|█▏  | 121/419.0 [00:10<00:24, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.350, EPE: 5.803:  29%|█▏  | 121/419.0 [00:10<00:24, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.350, EPE: 5.803:  29%|█▏  | 122/419.0 [00:10<00:24, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.350, EPE: 5.803:  29%|█▏  | 123/419.0 [00:10<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.350, EPE: 5.803:  29%|█▏  | 123/419.0 [00:10<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.350, EPE: 5.803:  29%|█▏  | 123/419.0 [00:10<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.346, EPE: 5.795:  30%|█▏  | 124/419.0 [00:11<00:24, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.346, EPE: 5.795:  30%|█▏  | 125/419.0 [00:11<00:24, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.346, EPE: 5.795:  30%|█▏  | 125/419.0 [00:11<00:24, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.342, EPE: 5.789:  30%|█▏  | 125/419.0 [00:11<00:24, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.344, EPE: 5.792:  30%|█▏  | 126/419.0 [00:11<00:24, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.344, EPE: 5.792:  30%|█▏  | 127/419.0 [00:11<00:24, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.344, EPE: 5.792:  30%|█▏  | 127/419.0 [00:11<00:24, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.347, EPE: 5.797:  30%|█▏  | 127/419.0 [00:11<00:24, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.341, EPE: 5.787:  31%|█▏  | 128/419.0 [00:11<00:24, 12.07it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.341, EPE: 5.787:  31%|█▏  | 129/419.0 [00:11<00:23, 12.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.341, EPE: 5.787:  31%|█▏  | 129/419.0 [00:11<00:23, 12.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.336, EPE: 5.778:  31%|█▏  | 129/419.0 [00:11<00:23, 12.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.335, EPE: 5.776:  31%|█▏  | 130/419.0 [00:11<00:23, 12.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.335, EPE: 5.776:  31%|█▎  | 131/419.0 [00:11<00:23, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.335, EPE: 5.776:  31%|█▎  | 131/419.0 [00:11<00:23, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.330, EPE: 5.768:  31%|█▎  | 131/419.0 [00:11<00:23, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.325, EPE: 5.759:  32%|█▎  | 132/419.0 [00:11<00:23, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.325, EPE: 5.759:  32%|█▎  | 133/419.0 [00:11<00:23, 12.19it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.325, EPE: 5.759:  32%|█▎  | 133/419.0 [00:11<00:23, 12.19it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.322, EPE: 5.754:  32%|█▎  | 133/419.0 [00:11<00:23, 12.19it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.326, EPE: 5.761:  32%|█▎  | 134/419.0 [00:11<00:23, 12.19it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.326, EPE: 5.761:  32%|█▎  | 135/419.0 [00:11<00:23, 12.25it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.326, EPE: 5.761:  32%|█▎  | 135/419.0 [00:11<00:23, 12.25it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.352, EPE: 5.806:  32%|█▎  | 135/419.0 [00:11<00:23, 12.25it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.400, EPE: 5.890:  32%|█▎  | 136/419.0 [00:12<00:23, 12.25it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.400, EPE: 5.890:  33%|█▎  | 137/419.0 [00:12<00:22, 12.28it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.400, EPE: 5.890:  33%|█▎  | 137/419.0 [00:12<00:22, 12.28it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.958:  33%|█▎  | 137/419.0 [00:12<00:22, 12.28it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.463, EPE: 5.997:  33%|█▎  | 138/419.0 [00:12<00:22, 12.28it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.463, EPE: 5.997:  33%|█▎  | 139/419.0 [00:12<00:22, 12.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.463, EPE: 5.997:  33%|█▎  | 139/419.0 [00:12<00:22, 12.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  33%|█▎  | 139/419.0 [00:12<00:22, 12.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.476, EPE: 6.021:  33%|█▎  | 140/419.0 [00:12<00:22, 12.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.476, EPE: 6.021:  34%|█▎  | 141/419.0 [00:12<00:22, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.476, EPE: 6.021:  34%|█▎  | 141/419.0 [00:12<00:22, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.480, EPE: 6.027:  34%|█▎  | 141/419.0 [00:12<00:22, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.030:  34%|█▎  | 142/419.0 [00:12<00:22, 12.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.030:  34%|█▎  | 143/419.0 [00:12<00:22, 12.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.030:  34%|█▎  | 143/419.0 [00:12<00:22, 12.14it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.482, EPE: 6.030:  34%|█▎  | 143/419.0 [00:12<00:22, 12.14it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.030:  34%|█▎  | 144/419.0 [00:12<00:22, 12.14it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.030:  35%|█▍  | 145/419.0 [00:12<00:22, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.030:  35%|█▍  | 145/419.0 [00:12<00:22, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.495, EPE: 6.053:  35%|█▍  | 145/419.0 [00:12<00:22, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.514, EPE: 6.086:  35%|█▍  | 146/419.0 [00:12<00:22, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.514, EPE: 6.086:  35%|█▍  | 147/419.0 [00:12<00:22, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.514, EPE: 6.086:  35%|█▍  | 147/419.0 [00:12<00:22, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.528, EPE: 6.111:  35%|█▍  | 147/419.0 [00:12<00:22, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.532, EPE: 6.118:  35%|█▍  | 148/419.0 [00:13<00:22, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.532, EPE: 6.118:  36%|█▍  | 149/419.0 [00:13<00:22, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.532, EPE: 6.118:  36%|█▍  | 149/419.0 [00:13<00:22, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.531, EPE: 6.116:  36%|█▍  | 149/419.0 [00:13<00:22, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.525, EPE: 6.106:  36%|█▍  | 150/419.0 [00:13<00:22, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.525, EPE: 6.106:  36%|█▍  | 151/419.0 [00:13<00:22, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.525, EPE: 6.106:  36%|█▍  | 151/419.0 [00:13<00:22, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.532, EPE: 6.118:  36%|█▍  | 151/419.0 [00:13<00:22, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.544, EPE: 6.138:  36%|█▍  | 152/419.0 [00:13<00:22, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.544, EPE: 6.138:  37%|█▍  | 153/419.0 [00:13<00:22, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.544, EPE: 6.138:  37%|█▍  | 153/419.0 [00:13<00:22, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.550, EPE: 6.149:  37%|█▍  | 153/419.0 [00:13<00:22, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.559, EPE: 6.164:  37%|█▍  | 154/419.0 [00:13<00:22, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.559, EPE: 6.164:  37%|█▍  | 155/419.0 [00:13<00:22, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.559, EPE: 6.164:  37%|█▍  | 155/419.0 [00:13<00:22, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.562, EPE: 6.169:  37%|█▍  | 155/419.0 [00:13<00:22, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.552, EPE: 6.153:  37%|█▍  | 156/419.0 [00:13<00:21, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.552, EPE: 6.153:  37%|█▍  | 157/419.0 [00:13<00:21, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.552, EPE: 6.153:  37%|█▍  | 157/419.0 [00:13<00:21, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.538, EPE: 6.128:  37%|█▍  | 157/419.0 [00:13<00:21, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.523, EPE: 6.103:  38%|█▌  | 158/419.0 [00:13<00:21, 12.06it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.523, EPE: 6.103:  38%|█▌  | 159/419.0 [00:13<00:21, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.523, EPE: 6.103:  38%|█▌  | 159/419.0 [00:13<00:21, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.513, EPE: 6.085:  38%|█▌  | 159/419.0 [00:13<00:21, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.507, EPE: 6.075:  38%|█▌  | 160/419.0 [00:14<00:21, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.507, EPE: 6.075:  38%|█▌  | 161/419.0 [00:14<00:21, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.507, EPE: 6.075:  38%|█▌  | 161/419.0 [00:14<00:21, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.499, EPE: 6.061:  38%|█▌  | 161/419.0 [00:14<00:21, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.489, EPE: 6.043:  39%|█▌  | 162/419.0 [00:14<00:21, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.489, EPE: 6.043:  39%|█▌  | 163/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.489, EPE: 6.043:  39%|█▌  | 163/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.477, EPE: 6.023:  39%|█▌  | 163/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  39%|█▌  | 164/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  39%|█▌  | 165/419.0 [00:14<00:21, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  39%|█▌  | 165/419.0 [00:14<00:21, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.459, EPE: 5.991:  39%|█▌  | 165/419.0 [00:14<00:21, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.450, EPE: 5.975:  40%|█▌  | 166/419.0 [00:14<00:21, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.450, EPE: 5.975:  40%|█▌  | 167/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.450, EPE: 5.975:  40%|█▌  | 167/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.444, EPE: 5.966:  40%|█▌  | 167/419.0 [00:14<00:21, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.439, EPE: 5.957:  40%|█▌  | 168/419.0 [00:14<00:20, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.439, EPE: 5.957:  40%|█▌  | 169/419.0 [00:14<00:20, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.439, EPE: 5.957:  40%|█▌  | 169/419.0 [00:14<00:20, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.438, EPE: 5.954:  40%|█▌  | 169/419.0 [00:14<00:20, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.959:  41%|█▌  | 170/419.0 [00:14<00:20, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.959:  41%|█▋  | 171/419.0 [00:14<00:20, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.440, EPE: 5.959:  41%|█▋  | 171/419.0 [00:14<00:20, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.453, EPE: 5.981:  41%|█▋  | 171/419.0 [00:14<00:20, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  41%|█▋  | 172/419.0 [00:15<00:20, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  41%|█▋  | 173/419.0 [00:15<00:20, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.469, EPE: 6.009:  41%|█▋  | 173/419.0 [00:15<00:20, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.481, EPE: 6.029:  41%|█▋  | 173/419.0 [00:15<00:20, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.493, EPE: 6.050:  42%|█▋  | 174/419.0 [00:15<00:20, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.493, EPE: 6.050:  42%|█▋  | 175/419.0 [00:15<00:20, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.493, EPE: 6.050:  42%|█▋  | 175/419.0 [00:15<00:20, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.504, EPE: 6.070:  42%|█▋  | 175/419.0 [00:15<00:20, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.518, EPE: 6.093:  42%|█▋  | 176/419.0 [00:15<00:20, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.518, EPE: 6.093:  42%|█▋  | 177/419.0 [00:15<00:20, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.518, EPE: 6.093:  42%|█▋  | 177/419.0 [00:15<00:20, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.531, EPE: 6.115:  42%|█▋  | 177/419.0 [00:15<00:20, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.540, EPE: 6.132:  42%|█▋  | 178/419.0 [00:15<00:20, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.540, EPE: 6.132:  43%|█▋  | 179/419.0 [00:15<00:20, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.540, EPE: 6.132:  43%|█▋  | 179/419.0 [00:15<00:20, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.546, EPE: 6.141:  43%|█▋  | 179/419.0 [00:15<00:20, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.543, EPE: 6.136:  43%|█▋  | 180/419.0 [00:15<00:20, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.543, EPE: 6.136:  43%|█▋  | 181/419.0 [00:15<00:20, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.543, EPE: 6.136:  43%|█▋  | 181/419.0 [00:15<00:20, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.535, EPE: 6.122:  43%|█▋  | 181/419.0 [00:15<00:20, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.531, EPE: 6.116:  43%|█▋  | 182/419.0 [00:15<00:19, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.531, EPE: 6.116:  44%|█▋  | 183/419.0 [00:15<00:19, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.531, EPE: 6.116:  44%|█▋  | 183/419.0 [00:15<00:19, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.524, EPE: 6.104:  44%|█▋  | 183/419.0 [00:15<00:19, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.510, EPE: 6.079:  44%|█▊  | 184/419.0 [00:16<00:19, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.510, EPE: 6.079:  44%|█▊  | 185/419.0 [00:16<00:19, 12.09it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.510, EPE: 6.079:  44%|█▊  | 185/419.0 [00:16<00:19, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.496, EPE: 6.056:  44%|█▊  | 185/419.0 [00:16<00:19, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.485, EPE: 6.036:  44%|█▊  | 186/419.0 [00:16<00:19, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.485, EPE: 6.036:  45%|█▊  | 187/419.0 [00:16<00:19, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.485, EPE: 6.036:  45%|█▊  | 187/419.0 [00:16<00:19, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.473, EPE: 6.016:  45%|█▊  | 187/419.0 [00:16<00:19, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.458, EPE: 5.990:  45%|█▊  | 188/419.0 [00:16<00:19, 12.10it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.458, EPE: 5.990:  45%|█▊  | 189/419.0 [00:16<00:19, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.458, EPE: 5.990:  45%|█▊  | 189/419.0 [00:16<00:19, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.442, EPE: 5.961:  45%|█▊  | 189/419.0 [00:16<00:19, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.425, EPE: 5.933:  45%|█▊  | 190/419.0 [00:16<00:18, 12.08it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.425, EPE: 5.933:  46%|█▊  | 191/419.0 [00:16<00:18, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.425, EPE: 5.933:  46%|█▊  | 191/419.0 [00:16<00:18, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.411, EPE: 5.908:  46%|█▊  | 191/419.0 [00:16<00:18, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.399, EPE: 5.887:  46%|█▊  | 192/419.0 [00:16<00:18, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.399, EPE: 5.887:  46%|█▊  | 193/419.0 [00:16<00:18, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.399, EPE: 5.887:  46%|█▊  | 193/419.0 [00:16<00:18, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.385, EPE: 5.863:  46%|█▊  | 193/419.0 [00:16<00:18, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.369, EPE: 5.835:  46%|█▊  | 194/419.0 [00:16<00:18, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.369, EPE: 5.835:  47%|█▊  | 195/419.0 [00:16<00:18, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.369, EPE: 5.835:  47%|█▊  | 195/419.0 [00:16<00:18, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.353, EPE: 5.808:  47%|█▊  | 195/419.0 [00:16<00:18, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.339, EPE: 5.783:  47%|█▊  | 196/419.0 [00:17<00:18, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.339, EPE: 5.783:  47%|█▉  | 197/419.0 [00:17<00:18, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.339, EPE: 5.783:  47%|█▉  | 197/419.0 [00:17<00:18, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.326, EPE: 5.761:  47%|█▉  | 197/419.0 [00:17<00:18, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.312, EPE: 5.737:  47%|█▉  | 198/419.0 [00:17<00:18, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.312, EPE: 5.737:  47%|█▉  | 199/419.0 [00:17<00:18, 11.98it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.312, EPE: 5.737:  47%|█▉  | 199/419.0 [00:17<00:18, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.298, EPE: 5.713:  47%|█▉  | 199/419.0 [00:17<00:18, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.287, EPE: 5.693:  48%|█▉  | 200/419.0 [00:17<00:18, 11.97it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.287, EPE: 5.693:  48%|█▉  | 201/419.0 [00:17<00:18, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.287, EPE: 5.693:  48%|█▉  | 201/419.0 [00:17<00:18, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.276, EPE: 5.674:  48%|█▉  | 201/419.0 [00:17<00:18, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.264, EPE: 5.653:  48%|█▉  | 202/419.0 [00:17<00:18, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.264, EPE: 5.653:  48%|█▉  | 203/419.0 [00:17<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.264, EPE: 5.653:  48%|█▉  | 203/419.0 [00:17<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.250, EPE: 5.629:  48%|█▉  | 203/419.0 [00:17<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.237, EPE: 5.607:  49%|█▉  | 204/419.0 [00:17<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.237, EPE: 5.607:  49%|█▉  | 205/419.0 [00:17<00:17, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.237, EPE: 5.607:  49%|█▉  | 205/419.0 [00:17<00:17, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.223, EPE: 5.582:  49%|█▉  | 205/419.0 [00:17<00:17, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.210, EPE: 5.560:  49%|█▉  | 206/419.0 [00:17<00:17, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.210, EPE: 5.560:  49%|█▉  | 207/419.0 [00:17<00:17, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.210, EPE: 5.560:  49%|█▉  | 207/419.0 [00:17<00:17, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.199, EPE: 5.540:  49%|█▉  | 207/419.0 [00:17<00:17, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.188, EPE: 5.522:  50%|█▉  | 208/419.0 [00:18<00:17, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.188, EPE: 5.522:  50%|█▉  | 209/419.0 [00:18<00:17, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.188, EPE: 5.522:  50%|█▉  | 209/419.0 [00:18<00:17, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.176, EPE: 5.501:  50%|█▉  | 209/419.0 [00:18<00:17, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.165, EPE: 5.481:  50%|██  | 210/419.0 [00:18<00:17, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.165, EPE: 5.481:  50%|██  | 211/419.0 [00:18<00:17, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.165, EPE: 5.481:  50%|██  | 211/419.0 [00:18<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.151, EPE: 5.457:  50%|██  | 211/419.0 [00:18<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.137, EPE: 5.433:  51%|██  | 212/419.0 [00:18<00:17, 12.01it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.137, EPE: 5.433:  51%|██  | 213/419.0 [00:18<00:17, 11.67it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.137, EPE: 5.433:  51%|██  | 213/419.0 [00:18<00:17, 11.67it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.124, EPE: 5.411:  51%|██  | 213/419.0 [00:18<00:17, 11.67it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.111, EPE: 5.388:  51%|██  | 214/419.0 [00:18<00:17, 11.67it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.111, EPE: 5.388:  51%|██  | 215/419.0 [00:18<00:17, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.111, EPE: 5.388:  51%|██  | 215/419.0 [00:18<00:17, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.097, EPE: 5.364:  51%|██  | 215/419.0 [00:18<00:17, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.083, EPE: 5.341:  52%|██  | 216/419.0 [00:18<00:17, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.083, EPE: 5.341:  52%|██  | 217/419.0 [00:18<00:17, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.083, EPE: 5.341:  52%|██  | 217/419.0 [00:18<00:17, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.070, EPE: 5.318:  52%|██  | 217/419.0 [00:18<00:17, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.057, EPE: 5.294:  52%|██  | 218/419.0 [00:18<00:17, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.057, EPE: 5.294:  52%|██  | 219/419.0 [00:18<00:17, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.057, EPE: 5.294:  52%|██  | 219/419.0 [00:18<00:17, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.044, EPE: 5.272:  52%|██  | 219/419.0 [00:18<00:17, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.031, EPE: 5.250:  53%|██  | 220/419.0 [00:19<00:16, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.031, EPE: 5.250:  53%|██  | 221/419.0 [00:19<00:16, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.031, EPE: 5.250:  53%|██  | 221/419.0 [00:19<00:16, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.018, EPE: 5.227:  53%|██  | 221/419.0 [00:19<00:16, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.004, EPE: 5.204:  53%|██  | 222/419.0 [00:19<00:16, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.004, EPE: 5.204:  53%|██▏ | 223/419.0 [00:19<00:16, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 3.004, EPE: 5.204:  53%|██▏ | 223/419.0 [00:19<00:16, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.991, EPE: 5.181:  53%|██▏ | 223/419.0 [00:19<00:16, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.979, EPE: 5.159:  53%|██▏ | 224/419.0 [00:19<00:16, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.979, EPE: 5.159:  54%|██▏ | 225/419.0 [00:19<00:16, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.979, EPE: 5.159:  54%|██▏ | 225/419.0 [00:19<00:16, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.968, EPE: 5.141:  54%|██▏ | 225/419.0 [00:19<00:16, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.957, EPE: 5.122:  54%|██▏ | 226/419.0 [00:19<00:16, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.957, EPE: 5.122:  54%|██▏ | 227/419.0 [00:19<00:16, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.957, EPE: 5.122:  54%|██▏ | 227/419.0 [00:19<00:16, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.945, EPE: 5.101:  54%|██▏ | 227/419.0 [00:19<00:16, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.934, EPE: 5.082:  54%|██▏ | 228/419.0 [00:19<00:16, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.934, EPE: 5.082:  55%|██▏ | 229/419.0 [00:19<00:15, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.934, EPE: 5.082:  55%|██▏ | 229/419.0 [00:19<00:15, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.923, EPE: 5.063:  55%|██▏ | 229/419.0 [00:19<00:15, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.913, EPE: 5.045:  55%|██▏ | 230/419.0 [00:19<00:15, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.913, EPE: 5.045:  55%|██▏ | 231/419.0 [00:19<00:15, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.913, EPE: 5.045:  55%|██▏ | 231/419.0 [00:19<00:15, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.903, EPE: 5.029:  55%|██▏ | 231/419.0 [00:19<00:15, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.893, EPE: 5.011:  55%|██▏ | 232/419.0 [00:20<00:15, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.893, EPE: 5.011:  56%|██▏ | 233/419.0 [00:20<00:15, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.893, EPE: 5.011:  56%|██▏ | 233/419.0 [00:20<00:15, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.882, EPE: 4.992:  56%|██▏ | 233/419.0 [00:20<00:15, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.871, EPE: 4.972:  56%|██▏ | 234/419.0 [00:20<00:15, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.871, EPE: 4.972:  56%|██▏ | 235/419.0 [00:20<00:15, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.871, EPE: 4.972:  56%|██▏ | 235/419.0 [00:20<00:15, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.859, EPE: 4.953:  56%|██▏ | 235/419.0 [00:20<00:15, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.848, EPE: 4.932:  56%|██▎ | 236/419.0 [00:20<00:15, 11.92it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.848, EPE: 4.932:  57%|██▎ | 237/419.0 [00:20<00:15, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.848, EPE: 4.932:  57%|██▎ | 237/419.0 [00:20<00:15, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.837, EPE: 4.914:  57%|██▎ | 237/419.0 [00:20<00:15, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.827, EPE: 4.896:  57%|██▎ | 238/419.0 [00:20<00:15, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.827, EPE: 4.896:  57%|██▎ | 239/419.0 [00:20<00:15, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.827, EPE: 4.896:  57%|██▎ | 239/419.0 [00:20<00:15, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.816, EPE: 4.877:  57%|██▎ | 239/419.0 [00:20<00:15, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.805, EPE: 4.859:  57%|██▎ | 240/419.0 [00:20<00:15, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.805, EPE: 4.859:  58%|██▎ | 241/419.0 [00:20<00:15, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.805, EPE: 4.859:  58%|██▎ | 241/419.0 [00:20<00:15, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.796, EPE: 4.842:  58%|██▎ | 241/419.0 [00:20<00:15, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.786, EPE: 4.825:  58%|██▎ | 242/419.0 [00:20<00:15, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.786, EPE: 4.825:  58%|██▎ | 243/419.0 [00:20<00:15, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.786, EPE: 4.825:  58%|██▎ | 243/419.0 [00:20<00:15, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.776, EPE: 4.808:  58%|██▎ | 243/419.0 [00:20<00:15, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.766, EPE: 4.791:  58%|██▎ | 244/419.0 [00:21<00:14, 11.72it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.766, EPE: 4.791:  58%|██▎ | 245/419.0 [00:21<00:14, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.766, EPE: 4.791:  58%|██▎ | 245/419.0 [00:21<00:14, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.756, EPE: 4.773:  58%|██▎ | 245/419.0 [00:21<00:14, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.746, EPE: 4.757:  59%|██▎ | 246/419.0 [00:21<00:14, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.746, EPE: 4.757:  59%|██▎ | 247/419.0 [00:21<00:14, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.746, EPE: 4.757:  59%|██▎ | 247/419.0 [00:21<00:14, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.736, EPE: 4.739:  59%|██▎ | 247/419.0 [00:21<00:14, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.727, EPE: 4.723:  59%|██▎ | 248/419.0 [00:21<00:14, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.727, EPE: 4.723:  59%|██▍ | 249/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.727, EPE: 4.723:  59%|██▍ | 249/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.717, EPE: 4.706:  59%|██▍ | 249/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.707, EPE: 4.689:  60%|██▍ | 250/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.707, EPE: 4.689:  60%|██▍ | 251/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.707, EPE: 4.689:  60%|██▍ | 251/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.697, EPE: 4.671:  60%|██▍ | 251/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.686, EPE: 4.653:  60%|██▍ | 252/419.0 [00:21<00:14, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.686, EPE: 4.653:  60%|██▍ | 253/419.0 [00:21<00:14, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.686, EPE: 4.653:  60%|██▍ | 253/419.0 [00:21<00:14, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.678, EPE: 4.638:  60%|██▍ | 253/419.0 [00:21<00:14, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.668, EPE: 4.622:  61%|██▍ | 254/419.0 [00:21<00:14, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.668, EPE: 4.622:  61%|██▍ | 255/419.0 [00:21<00:13, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.668, EPE: 4.622:  61%|██▍ | 255/419.0 [00:21<00:13, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.659, EPE: 4.606:  61%|██▍ | 255/419.0 [00:21<00:13, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.652, EPE: 4.594:  61%|██▍ | 256/419.0 [00:22<00:13, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.652, EPE: 4.594:  61%|██▍ | 257/419.0 [00:22<00:13, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.652, EPE: 4.594:  61%|██▍ | 257/419.0 [00:22<00:13, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.647, EPE: 4.585:  61%|██▍ | 257/419.0 [00:22<00:13, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.639, EPE: 4.571:  62%|██▍ | 258/419.0 [00:22<00:13, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.639, EPE: 4.571:  62%|██▍ | 259/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.639, EPE: 4.571:  62%|██▍ | 259/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.631, EPE: 4.557:  62%|██▍ | 259/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.622, EPE: 4.542:  62%|██▍ | 260/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.622, EPE: 4.542:  62%|██▍ | 261/419.0 [00:22<00:13, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.622, EPE: 4.542:  62%|██▍ | 261/419.0 [00:22<00:13, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.615, EPE: 4.529:  62%|██▍ | 261/419.0 [00:22<00:13, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.608, EPE: 4.517:  63%|██▌ | 262/419.0 [00:22<00:13, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.608, EPE: 4.517:  63%|██▌ | 263/419.0 [00:22<00:13, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.608, EPE: 4.517:  63%|██▌ | 263/419.0 [00:22<00:13, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.601, EPE: 4.504:  63%|██▌ | 263/419.0 [00:22<00:13, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.594, EPE: 4.493:  63%|██▌ | 264/419.0 [00:22<00:13, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.594, EPE: 4.493:  63%|██▌ | 265/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.594, EPE: 4.493:  63%|██▌ | 265/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.589, EPE: 4.483:  63%|██▌ | 265/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.581, EPE: 4.470:  63%|██▌ | 266/419.0 [00:22<00:13, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.581, EPE: 4.470:  64%|██▌ | 267/419.0 [00:22<00:12, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.581, EPE: 4.470:  64%|██▌ | 267/419.0 [00:22<00:12, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.574, EPE: 4.458:  64%|██▌ | 267/419.0 [00:23<00:12, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.568, EPE: 4.448:  64%|██▌ | 268/419.0 [00:23<00:12, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.568, EPE: 4.448:  64%|██▌ | 269/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.568, EPE: 4.448:  64%|██▌ | 269/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.561, EPE: 4.436:  64%|██▌ | 269/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.555, EPE: 4.426:  64%|██▌ | 270/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.555, EPE: 4.426:  65%|██▌ | 271/419.0 [00:23<00:12, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.555, EPE: 4.426:  65%|██▌ | 271/419.0 [00:23<00:12, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.548, EPE: 4.413:  65%|██▌ | 271/419.0 [00:23<00:12, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.539, EPE: 4.398:  65%|██▌ | 272/419.0 [00:23<00:12, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.539, EPE: 4.398:  65%|██▌ | 273/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.539, EPE: 4.398:  65%|██▌ | 273/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.531, EPE: 4.384:  65%|██▌ | 273/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.523, EPE: 4.370:  65%|██▌ | 274/419.0 [00:23<00:12, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.523, EPE: 4.370:  66%|██▋ | 275/419.0 [00:23<00:12, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.523, EPE: 4.370:  66%|██▋ | 275/419.0 [00:23<00:12, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.515, EPE: 4.356:  66%|██▋ | 275/419.0 [00:23<00:12, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.506, EPE: 4.341:  66%|██▋ | 276/419.0 [00:23<00:11, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.506, EPE: 4.341:  66%|██▋ | 277/419.0 [00:23<00:12, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.506, EPE: 4.341:  66%|██▋ | 277/419.0 [00:23<00:12, 11.80it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.498, EPE: 4.327:  66%|██▋ | 277/419.0 [00:23<00:12, 11.80it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.491, EPE: 4.314:  66%|██▋ | 278/419.0 [00:23<00:11, 11.80it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.491, EPE: 4.314:  67%|██▋ | 279/419.0 [00:23<00:12, 11.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.491, EPE: 4.314:  67%|██▋ | 279/419.0 [00:23<00:12, 11.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.484, EPE: 4.302:  67%|██▋ | 279/419.0 [00:24<00:12, 11.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.478, EPE: 4.292:  67%|██▋ | 280/419.0 [00:24<00:12, 11.15it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.478, EPE: 4.292:  67%|██▋ | 281/419.0 [00:24<00:12, 11.17it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.478, EPE: 4.292:  67%|██▋ | 281/419.0 [00:24<00:12, 11.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.470, EPE: 4.278:  67%|██▋ | 281/419.0 [00:24<00:12, 11.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.464, EPE: 4.268:  67%|██▋ | 282/419.0 [00:24<00:12, 11.16it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.464, EPE: 4.268:  68%|██▋ | 283/419.0 [00:24<00:12, 11.32it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.464, EPE: 4.268:  68%|██▋ | 283/419.0 [00:24<00:12, 11.32it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.460, EPE: 4.262:  68%|██▋ | 283/419.0 [00:24<00:12, 11.32it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.455, EPE: 4.252:  68%|██▋ | 284/419.0 [00:24<00:11, 11.32it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.455, EPE: 4.252:  68%|██▋ | 285/419.0 [00:24<00:11, 11.44it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.455, EPE: 4.252:  68%|██▋ | 285/419.0 [00:24<00:11, 11.43it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.450, EPE: 4.244:  68%|██▋ | 285/419.0 [00:24<00:11, 11.43it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.444, EPE: 4.234:  68%|██▋ | 286/419.0 [00:24<00:11, 11.43it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.444, EPE: 4.234:  68%|██▋ | 287/419.0 [00:24<00:11, 11.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.444, EPE: 4.234:  68%|██▋ | 287/419.0 [00:24<00:11, 11.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.438, EPE: 4.222:  68%|██▋ | 287/419.0 [00:24<00:11, 11.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.433, EPE: 4.213:  69%|██▋ | 288/419.0 [00:24<00:11, 11.49it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.433, EPE: 4.213:  69%|██▊ | 289/419.0 [00:24<00:11, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.433, EPE: 4.213:  69%|██▊ | 289/419.0 [00:24<00:11, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.427, EPE: 4.203:  69%|██▊ | 289/419.0 [00:24<00:11, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.420, EPE: 4.191:  69%|██▊ | 290/419.0 [00:25<00:11, 11.68it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.420, EPE: 4.191:  69%|██▊ | 291/419.0 [00:25<00:10, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.420, EPE: 4.191:  69%|██▊ | 291/419.0 [00:25<00:10, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.412, EPE: 4.178:  69%|██▊ | 291/419.0 [00:25<00:10, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.405, EPE: 4.166:  70%|██▊ | 292/419.0 [00:25<00:10, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.405, EPE: 4.166:  70%|██▊ | 293/419.0 [00:25<00:10, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.405, EPE: 4.166:  70%|██▊ | 293/419.0 [00:25<00:10, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.399, EPE: 4.154:  70%|██▊ | 293/419.0 [00:25<00:10, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.392, EPE: 4.142:  70%|██▊ | 294/419.0 [00:25<00:10, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.392, EPE: 4.142:  70%|██▊ | 295/419.0 [00:25<00:10, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.392, EPE: 4.142:  70%|██▊ | 295/419.0 [00:25<00:10, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.384, EPE: 4.130:  70%|██▊ | 295/419.0 [00:25<00:10, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.377, EPE: 4.117:  71%|██▊ | 296/419.0 [00:25<00:10, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.377, EPE: 4.117:  71%|██▊ | 297/419.0 [00:25<00:10, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.377, EPE: 4.117:  71%|██▊ | 297/419.0 [00:25<00:10, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.370, EPE: 4.105:  71%|██▊ | 297/419.0 [00:25<00:10, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.364, EPE: 4.094:  71%|██▊ | 298/419.0 [00:25<00:10, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.364, EPE: 4.094:  71%|██▊ | 299/419.0 [00:25<00:10, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.364, EPE: 4.094:  71%|██▊ | 299/419.0 [00:25<00:10, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.357, EPE: 4.082:  71%|██▊ | 299/419.0 [00:25<00:10, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.350, EPE: 4.071:  72%|██▊ | 300/419.0 [00:25<00:09, 11.95it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.350, EPE: 4.071:  72%|██▊ | 301/419.0 [00:25<00:09, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.350, EPE: 4.071:  72%|██▊ | 301/419.0 [00:25<00:09, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.344, EPE: 4.060:  72%|██▊ | 301/419.0 [00:25<00:09, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.339, EPE: 4.051:  72%|██▉ | 302/419.0 [00:26<00:09, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.339, EPE: 4.051:  72%|██▉ | 303/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.339, EPE: 4.051:  72%|██▉ | 303/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.333, EPE: 4.041:  72%|██▉ | 303/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.326, EPE: 4.028:  73%|██▉ | 304/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.326, EPE: 4.028:  73%|██▉ | 305/419.0 [00:26<00:09, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.326, EPE: 4.028:  73%|██▉ | 305/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.319, EPE: 4.016:  73%|██▉ | 305/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.312, EPE: 4.004:  73%|██▉ | 306/419.0 [00:26<00:09, 11.87it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.312, EPE: 4.004:  73%|██▉ | 307/419.0 [00:26<00:09, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.312, EPE: 4.004:  73%|██▉ | 307/419.0 [00:26<00:09, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.306, EPE: 3.994:  73%|██▉ | 307/419.0 [00:26<00:09, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.300, EPE: 3.983:  74%|██▉ | 308/419.0 [00:26<00:09, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.300, EPE: 3.983:  74%|██▉ | 309/419.0 [00:26<00:09, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.300, EPE: 3.983:  74%|██▉ | 309/419.0 [00:26<00:09, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.293, EPE: 3.972:  74%|██▉ | 309/419.0 [00:26<00:09, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.287, EPE: 3.961:  74%|██▉ | 310/419.0 [00:26<00:09, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.287, EPE: 3.961:  74%|██▉ | 311/419.0 [00:26<00:09, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.287, EPE: 3.961:  74%|██▉ | 311/419.0 [00:26<00:09, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.281, EPE: 3.951:  74%|██▉ | 311/419.0 [00:26<00:09, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.276, EPE: 3.942:  74%|██▉ | 312/419.0 [00:26<00:09, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.276, EPE: 3.942:  75%|██▉ | 313/419.0 [00:26<00:08, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.276, EPE: 3.942:  75%|██▉ | 313/419.0 [00:26<00:08, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.270, EPE: 3.932:  75%|██▉ | 313/419.0 [00:26<00:08, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.264, EPE: 3.921:  75%|██▉ | 314/419.0 [00:27<00:08, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.264, EPE: 3.921:  75%|███ | 315/419.0 [00:27<00:08, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.264, EPE: 3.921:  75%|███ | 315/419.0 [00:27<00:08, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.258, EPE: 3.910:  75%|███ | 315/419.0 [00:27<00:08, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.251, EPE: 3.899:  75%|███ | 316/419.0 [00:27<00:08, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.251, EPE: 3.899:  76%|███ | 317/419.0 [00:27<00:08, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.251, EPE: 3.899:  76%|███ | 317/419.0 [00:27<00:08, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.245, EPE: 3.888:  76%|███ | 317/419.0 [00:27<00:08, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.238, EPE: 3.877:  76%|███ | 318/419.0 [00:27<00:08, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.238, EPE: 3.877:  76%|███ | 319/419.0 [00:27<00:08, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.238, EPE: 3.877:  76%|███ | 319/419.0 [00:27<00:08, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.233, EPE: 3.868:  76%|███ | 319/419.0 [00:27<00:08, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.227, EPE: 3.858:  76%|███ | 320/419.0 [00:27<00:08, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.227, EPE: 3.858:  77%|███ | 321/419.0 [00:27<00:08, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.227, EPE: 3.858:  77%|███ | 321/419.0 [00:27<00:08, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.221, EPE: 3.847:  77%|███ | 321/419.0 [00:27<00:08, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.215, EPE: 3.836:  77%|███ | 322/419.0 [00:27<00:08, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.215, EPE: 3.836:  77%|███ | 323/419.0 [00:27<00:08, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.215, EPE: 3.836:  77%|███ | 323/419.0 [00:27<00:08, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.209, EPE: 3.826:  77%|███ | 323/419.0 [00:27<00:08, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.203, EPE: 3.815:  77%|███ | 324/419.0 [00:27<00:08, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.203, EPE: 3.815:  78%|███ | 325/419.0 [00:27<00:07, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.203, EPE: 3.815:  78%|███ | 325/419.0 [00:27<00:07, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.196, EPE: 3.804:  78%|███ | 325/419.0 [00:27<00:07, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.191, EPE: 3.794:  78%|███ | 326/419.0 [00:28<00:07, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.191, EPE: 3.794:  78%|███ | 327/419.0 [00:28<00:07, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.191, EPE: 3.794:  78%|███ | 327/419.0 [00:28<00:07, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.184, EPE: 3.783:  78%|███ | 327/419.0 [00:28<00:07, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.179, EPE: 3.775:  78%|███▏| 328/419.0 [00:28<00:07, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.179, EPE: 3.775:  79%|███▏| 329/419.0 [00:28<00:07, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.179, EPE: 3.775:  79%|███▏| 329/419.0 [00:28<00:07, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.174, EPE: 3.766:  79%|███▏| 329/419.0 [00:28<00:07, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.169, EPE: 3.757:  79%|███▏| 330/419.0 [00:28<00:07, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.169, EPE: 3.757:  79%|███▏| 331/419.0 [00:28<00:07, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.169, EPE: 3.757:  79%|███▏| 331/419.0 [00:28<00:07, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.164, EPE: 3.749:  79%|███▏| 331/419.0 [00:28<00:07, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.159, EPE: 3.739:  79%|███▏| 332/419.0 [00:28<00:07, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.159, EPE: 3.739:  79%|███▏| 333/419.0 [00:28<00:07, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.159, EPE: 3.739:  79%|███▏| 333/419.0 [00:28<00:07, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.153, EPE: 3.729:  79%|███▏| 333/419.0 [00:28<00:07, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.147, EPE: 3.719:  80%|███▏| 334/419.0 [00:28<00:07, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.147, EPE: 3.719:  80%|███▏| 335/419.0 [00:28<00:07, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.147, EPE: 3.719:  80%|███▏| 335/419.0 [00:28<00:07, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.142, EPE: 3.709:  80%|███▏| 335/419.0 [00:28<00:07, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.136, EPE: 3.700:  80%|███▏| 336/419.0 [00:28<00:07, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.136, EPE: 3.700:  80%|███▏| 337/419.0 [00:28<00:06, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.136, EPE: 3.700:  80%|███▏| 337/419.0 [00:28<00:06, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.132, EPE: 3.692:  80%|███▏| 337/419.0 [00:28<00:06, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.126, EPE: 3.683:  81%|███▏| 338/419.0 [00:29<00:06, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.126, EPE: 3.683:  81%|███▏| 339/419.0 [00:29<00:06, 11.89it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.126, EPE: 3.683:  81%|███▏| 339/419.0 [00:29<00:06, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.121, EPE: 3.674:  81%|███▏| 339/419.0 [00:29<00:06, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.116, EPE: 3.665:  81%|███▏| 340/419.0 [00:29<00:06, 11.88it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.116, EPE: 3.665:  81%|███▎| 341/419.0 [00:29<00:06, 11.83it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.116, EPE: 3.665:  81%|███▎| 341/419.0 [00:29<00:06, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.111, EPE: 3.657:  81%|███▎| 341/419.0 [00:29<00:06, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.106, EPE: 3.647:  82%|███▎| 342/419.0 [00:29<00:06, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.106, EPE: 3.647:  82%|███▎| 343/419.0 [00:29<00:06, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.106, EPE: 3.647:  82%|███▎| 343/419.0 [00:29<00:06, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.101, EPE: 3.639:  82%|███▎| 343/419.0 [00:29<00:06, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.097, EPE: 3.632:  82%|███▎| 344/419.0 [00:29<00:06, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.097, EPE: 3.632:  82%|███▎| 345/419.0 [00:29<00:06, 11.82it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.097, EPE: 3.632:  82%|███▎| 345/419.0 [00:29<00:06, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.093, EPE: 3.626:  82%|███▎| 345/419.0 [00:29<00:06, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.089, EPE: 3.619:  83%|███▎| 346/419.0 [00:29<00:06, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.089, EPE: 3.619:  83%|███▎| 347/419.0 [00:29<00:06, 11.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.089, EPE: 3.619:  83%|███▎| 347/419.0 [00:29<00:06, 11.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.085, EPE: 3.611:  83%|███▎| 347/419.0 [00:29<00:06, 11.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.080, EPE: 3.602:  83%|███▎| 348/419.0 [00:29<00:06, 11.20it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.080, EPE: 3.602:  83%|███▎| 349/419.0 [00:29<00:06, 11.19it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.080, EPE: 3.602:  83%|███▎| 349/419.0 [00:29<00:06, 11.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.074, EPE: 3.593:  83%|███▎| 349/419.0 [00:30<00:06, 11.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.069, EPE: 3.583:  84%|███▎| 350/419.0 [00:30<00:06, 11.18it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.069, EPE: 3.583:  84%|███▎| 351/419.0 [00:30<00:05, 11.34it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.069, EPE: 3.583:  84%|███▎| 351/419.0 [00:30<00:05, 11.34it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.063, EPE: 3.574:  84%|███▎| 351/419.0 [00:30<00:05, 11.34it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.058, EPE: 3.564:  84%|███▎| 352/419.0 [00:30<00:05, 11.34it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.058, EPE: 3.564:  84%|███▎| 353/419.0 [00:30<00:05, 11.41it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.058, EPE: 3.564:  84%|███▎| 353/419.0 [00:30<00:05, 11.41it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.052, EPE: 3.555:  84%|███▎| 353/419.0 [00:30<00:05, 11.41it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.048, EPE: 3.546:  84%|███▍| 354/419.0 [00:30<00:05, 11.41it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.048, EPE: 3.546:  85%|███▍| 355/419.0 [00:30<00:05, 11.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.048, EPE: 3.546:  85%|███▍| 355/419.0 [00:30<00:05, 11.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.043, EPE: 3.538:  85%|███▍| 355/419.0 [00:30<00:05, 11.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.038, EPE: 3.530:  85%|███▍| 356/419.0 [00:30<00:05, 11.55it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.038, EPE: 3.530:  85%|███▍| 357/419.0 [00:30<00:05, 11.40it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.038, EPE: 3.530:  85%|███▍| 357/419.0 [00:30<00:05, 11.40it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.033, EPE: 3.522:  85%|███▍| 357/419.0 [00:30<00:05, 11.40it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.030, EPE: 3.516:  85%|███▍| 358/419.0 [00:30<00:05, 11.40it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.030, EPE: 3.516:  86%|███▍| 359/419.0 [00:30<00:05, 11.57it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.030, EPE: 3.516:  86%|███▍| 359/419.0 [00:30<00:05, 11.56it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.027, EPE: 3.510:  86%|███▍| 359/419.0 [00:30<00:05, 11.56it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.023, EPE: 3.503:  86%|███▍| 360/419.0 [00:30<00:05, 11.56it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.023, EPE: 3.503:  86%|███▍| 361/419.0 [00:30<00:04, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.023, EPE: 3.503:  86%|███▍| 361/419.0 [00:30<00:04, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.018, EPE: 3.495:  86%|███▍| 361/419.0 [00:31<00:04, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.013, EPE: 3.486:  86%|███▍| 362/419.0 [00:31<00:04, 11.69it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.013, EPE: 3.486:  87%|███▍| 363/419.0 [00:31<00:04, 11.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.013, EPE: 3.486:  87%|███▍| 363/419.0 [00:31<00:04, 11.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.009, EPE: 3.479:  87%|███▍| 363/419.0 [00:31<00:04, 11.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.005, EPE: 3.472:  87%|███▍| 364/419.0 [00:31<00:04, 11.59it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.005, EPE: 3.472:  87%|███▍| 365/419.0 [00:31<00:04, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.005, EPE: 3.472:  87%|███▍| 365/419.0 [00:31<00:04, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.000, EPE: 3.464:  87%|███▍| 365/419.0 [00:31<00:04, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.995, EPE: 3.455:  87%|███▍| 366/419.0 [00:31<00:04, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.995, EPE: 3.455:  88%|███▌| 367/419.0 [00:31<00:04, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.995, EPE: 3.455:  88%|███▌| 367/419.0 [00:31<00:04, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.990, EPE: 3.447:  88%|███▌| 367/419.0 [00:31<00:04, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.985, EPE: 3.439:  88%|███▌| 368/419.0 [00:31<00:04, 11.75it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.985, EPE: 3.439:  88%|███▌| 369/419.0 [00:31<00:04, 11.80it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.985, EPE: 3.439:  88%|███▌| 369/419.0 [00:31<00:04, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.981, EPE: 3.431:  88%|███▌| 369/419.0 [00:31<00:04, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.978, EPE: 3.426:  88%|███▌| 370/419.0 [00:31<00:04, 11.79it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.978, EPE: 3.426:  89%|███▌| 371/419.0 [00:31<00:04, 11.91it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.978, EPE: 3.426:  89%|███▌| 371/419.0 [00:31<00:04, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.977, EPE: 3.424:  89%|███▌| 371/419.0 [00:31<00:04, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.978, EPE: 3.426:  89%|███▌| 372/419.0 [00:31<00:03, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.978, EPE: 3.426:  89%|███▌| 373/419.0 [00:31<00:03, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.978, EPE: 3.426:  89%|███▌| 373/419.0 [00:31<00:03, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.981, EPE: 3.431:  89%|███▌| 373/419.0 [00:32<00:03, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.985, EPE: 3.438:  89%|███▌| 374/419.0 [00:32<00:03, 11.90it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.985, EPE: 3.438:  89%|███▌| 375/419.0 [00:32<00:03, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.985, EPE: 3.438:  89%|███▌| 375/419.0 [00:32<00:03, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.991, EPE: 3.448:  89%|███▌| 375/419.0 [00:32<00:03, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.995, EPE: 3.456:  90%|███▌| 376/419.0 [00:32<00:03, 11.86it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.995, EPE: 3.456:  90%|███▌| 377/419.0 [00:32<00:03, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.995, EPE: 3.456:  90%|███▌| 377/419.0 [00:32<00:03, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.999, EPE: 3.463:  90%|███▌| 377/419.0 [00:32<00:03, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.002, EPE: 3.468:  90%|███▌| 378/419.0 [00:32<00:03, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.002, EPE: 3.468:  90%|███▌| 379/419.0 [00:32<00:03, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.002, EPE: 3.468:  90%|███▌| 379/419.0 [00:32<00:03, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.004, EPE: 3.471:  90%|███▌| 379/419.0 [00:32<00:03, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.004, EPE: 3.472:  91%|███▋| 380/419.0 [00:32<00:03, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.004, EPE: 3.472:  91%|███▋| 381/419.0 [00:32<00:03, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.004, EPE: 3.472:  91%|███▋| 381/419.0 [00:32<00:03, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.002, EPE: 3.468:  91%|███▋| 381/419.0 [00:32<00:03, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.998, EPE: 3.461:  91%|███▋| 382/419.0 [00:32<00:03, 11.74it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.998, EPE: 3.461:  91%|███▋| 383/419.0 [00:32<00:03, 11.77it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.998, EPE: 3.461:  91%|███▋| 383/419.0 [00:32<00:03, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 1.998, EPE: 3.461:  91%|███▋| 383/419.0 [00:32<00:03, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.003, EPE: 3.470:  92%|███▋| 384/419.0 [00:33<00:02, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.003, EPE: 3.470:  92%|███▋| 385/419.0 [00:33<00:02, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.003, EPE: 3.470:  92%|███▋| 385/419.0 [00:33<00:02, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.014, EPE: 3.488:  92%|███▋| 385/419.0 [00:33<00:02, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.027, EPE: 3.511:  92%|███▋| 386/419.0 [00:33<00:02, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.027, EPE: 3.511:  92%|███▋| 387/419.0 [00:33<00:02, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.027, EPE: 3.511:  92%|███▋| 387/419.0 [00:33<00:02, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.040, EPE: 3.533:  92%|███▋| 387/419.0 [00:33<00:02, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.052, EPE: 3.554:  93%|███▋| 388/419.0 [00:33<00:02, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.052, EPE: 3.554:  93%|███▋| 389/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.052, EPE: 3.554:  93%|███▋| 389/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.063, EPE: 3.573:  93%|███▋| 389/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.076, EPE: 3.596:  93%|███▋| 390/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.076, EPE: 3.596:  93%|███▋| 391/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.076, EPE: 3.596:  93%|███▋| 391/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.093, EPE: 3.625:  93%|███▋| 391/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.108, EPE: 3.650:  94%|███▋| 392/419.0 [00:33<00:02, 11.85it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.108, EPE: 3.650:  94%|███▊| 393/419.0 [00:33<00:02, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.108, EPE: 3.650:  94%|███▊| 393/419.0 [00:33<00:02, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.118, EPE: 3.669:  94%|███▊| 393/419.0 [00:33<00:02, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.129, EPE: 3.687:  94%|███▊| 394/419.0 [00:33<00:02, 11.81it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.129, EPE: 3.687:  94%|███▊| 395/419.0 [00:33<00:02, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.129, EPE: 3.687:  94%|███▊| 395/419.0 [00:33<00:02, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.139, EPE: 3.706:  94%|███▊| 395/419.0 [00:33<00:02, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.147, EPE: 3.719:  95%|███▊| 396/419.0 [00:34<00:01, 11.84it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.147, EPE: 3.719:  95%|███▊| 397/419.0 [00:34<00:01, 11.71it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.147, EPE: 3.719:  95%|███▊| 397/419.0 [00:34<00:01, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.164, EPE: 3.749:  95%|███▊| 397/419.0 [00:34<00:01, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.192, EPE: 3.797:  95%|███▊| 398/419.0 [00:34<00:01, 11.70it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.192, EPE: 3.797:  95%|███▊| 399/419.0 [00:34<00:01, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.192, EPE: 3.797:  95%|███▊| 399/419.0 [00:34<00:01, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.224, EPE: 3.851:  95%|███▊| 399/419.0 [00:34<00:01, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.250, EPE: 3.898:  95%|███▊| 400/419.0 [00:34<00:01, 11.78it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.250, EPE: 3.898:  96%|███▊| 401/419.0 [00:34<00:01, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.250, EPE: 3.898:  96%|███▊| 401/419.0 [00:34<00:01, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.273, EPE: 3.938:  96%|███▊| 401/419.0 [00:34<00:01, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.292, EPE: 3.970:  96%|███▊| 402/419.0 [00:34<00:01, 11.73it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.292, EPE: 3.970:  96%|███▊| 403/419.0 [00:34<00:01, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.292, EPE: 3.970:  96%|███▊| 403/419.0 [00:34<00:01, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.307, EPE: 3.996:  96%|███▊| 403/419.0 [00:34<00:01, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.324, EPE: 4.026:  96%|███▊| 404/419.0 [00:34<00:01, 11.76it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.324, EPE: 4.026:  97%|███▊| 405/419.0 [00:34<00:01, 11.94it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.324, EPE: 4.026:  97%|███▊| 405/419.0 [00:34<00:01, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.344, EPE: 4.059:  97%|███▊| 405/419.0 [00:34<00:01, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.363, EPE: 4.092:  97%|███▉| 406/419.0 [00:34<00:01, 11.93it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.363, EPE: 4.092:  97%|███▉| 407/419.0 [00:34<00:00, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.363, EPE: 4.092:  97%|███▉| 407/419.0 [00:34<00:00, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.382, EPE: 4.126:  97%|███▉| 407/419.0 [00:34<00:00, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.398, EPE: 4.153:  97%|███▉| 408/419.0 [00:35<00:00, 12.05it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.398, EPE: 4.153:  98%|███▉| 409/419.0 [00:35<00:00, 12.13it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.398, EPE: 4.153:  98%|███▉| 409/419.0 [00:35<00:00, 12.13it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.415, EPE: 4.182:  98%|███▉| 409/419.0 [00:35<00:00, 12.13it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.430, EPE: 4.209:  98%|███▉| 410/419.0 [00:35<00:00, 12.13it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.430, EPE: 4.209:  98%|███▉| 411/419.0 [00:35<00:00, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.430, EPE: 4.209:  98%|███▉| 411/419.0 [00:35<00:00, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.444, EPE: 4.233:  98%|███▉| 411/419.0 [00:35<00:00, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.456, EPE: 4.254:  98%|███▉| 412/419.0 [00:35<00:00, 12.03it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.456, EPE: 4.254:  99%|███▉| 413/419.0 [00:35<00:00, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.456, EPE: 4.254:  99%|███▉| 413/419.0 [00:35<00:00, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.468, EPE: 4.274:  99%|███▉| 413/419.0 [00:35<00:00, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.481, EPE: 4.298:  99%|███▉| 414/419.0 [00:35<00:00, 12.02it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.481, EPE: 4.298:  99%|███▉| 415/419.0 [00:35<00:00, 12.00it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.481, EPE: 4.298:  99%|███▉| 415/419.0 [00:35<00:00, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.496, EPE: 4.323:  99%|███▉| 415/419.0 [00:35<00:00, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.510, EPE: 4.348:  99%|███▉| 416/419.0 [00:35<00:00, 11.99it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.510, EPE: 4.348: 100%|███▉| 417/419.0 [00:35<00:00, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.510, EPE: 4.348: 100%|███▉| 417/419.0 [00:35<00:00, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.527, EPE: 4.377: 100%|███▉| 417/419.0 [00:35<00:00, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.542, EPE: 4.403: 100%|███▉| 418/419.0 [00:35<00:00, 12.04it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.542, EPE: 4.403: 100%|████| 419/419.0 [00:35<00:00, 12.12it/s]\u001b[A\n","Inference Averages for Epoch 0: L1: 2.542, EPE: 4.403: 100%|████| 419/419.0 [00:35<00:00, 11.64it/s]\n","Overall Progress: 100%|███████████████████████████████████████████████| 1/1 [00:35<00:00, 35.98s/it]\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DoqAzcM-E2_f"},"source":["# import shutil\n","# shutil.rmtree('/content/flownet2-pytorch/networks/resample2d_package/resample2d_cuda.egg-info')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_rqB6aLFhCl"},"source":["### How to get the .flo files to work."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GI-j9SQAC_CG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613298978509,"user_tz":-60,"elapsed":87946,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"f68a1c4d-5402-48cd-cf83-840c3582508a"},"source":["!zip -r /content/flow_files_anon001.zip /content/flownet2-pytorch/results/inference/run.epoch-0-flow-field"],"execution_count":21,"outputs":[{"output_type":"stream","text":["  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/ (stored 0%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000276.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000310.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000024.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000363.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000021.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000413.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000376.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000254.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000313.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000280.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000034.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000022.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000014.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000132.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000289.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000123.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000410.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000181.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000327.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000336.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000153.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000131.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000333.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000249.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000162.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000403.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000071.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000074.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000290.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000105.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000309.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000194.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000043.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000373.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000275.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000304.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000383.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000054.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000405.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000241.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000081.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000102.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000205.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000279.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000206.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000049.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000091.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000232.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000113.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000340.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000183.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000164.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000371.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000202.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000364.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000177.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000390.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000031.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000055.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000107.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000399.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000053.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000068.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000045.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000255.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000303.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000168.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000272.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000028.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000415.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000083.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000259.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000349.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000305.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000199.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000251.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000302.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000051.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000211.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000321.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000311.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000372.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000019.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000360.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000149.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000291.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000224.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000174.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000062.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000145.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000215.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000015.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000354.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000237.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000318.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000359.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000086.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000409.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000073.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000161.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000186.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000126.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000380.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000023.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000328.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000025.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000343.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000069.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000163.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000270.flo (deflated 14%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000314.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000143.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000212.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000353.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000013.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000295.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000418.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000286.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000386.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000152.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000048.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000352.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000358.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000001.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000110.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000361.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000417.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000089.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000008.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000100.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000003.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000217.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000016.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000227.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000334.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000124.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000142.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000345.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000171.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000104.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000393.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000225.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000027.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000325.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000018.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000103.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000002.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000114.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000172.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000406.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000292.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000220.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000198.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000156.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000158.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000274.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000208.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000146.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000188.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000207.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000414.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000037.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000005.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000326.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000278.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000209.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000287.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000389.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000228.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000012.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000010.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000297.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000400.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000139.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000106.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000344.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000234.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000119.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000094.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000324.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000121.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000260.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000323.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000185.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000088.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000115.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000213.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000317.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000167.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000148.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000411.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000294.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000338.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000362.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000381.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000067.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000160.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000401.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000271.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000178.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000412.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000087.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000109.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000316.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000322.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000233.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000041.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000137.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000173.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000306.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000298.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000312.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000252.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000009.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000046.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000293.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000092.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000283.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000367.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000057.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000357.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000350.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000301.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000346.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000120.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000150.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000320.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000078.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000159.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000246.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000236.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000201.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000261.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000155.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000082.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000065.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000176.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000277.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000117.flo (deflated 14%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000076.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000266.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000284.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000190.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000192.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000144.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000058.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000394.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000404.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000307.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000112.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000257.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000140.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000247.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000378.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000370.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000017.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000042.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000300.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000006.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000282.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000070.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000369.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000308.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000077.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000127.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000238.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000243.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000288.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000214.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000175.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000169.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000072.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000154.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000182.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000180.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000075.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000166.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000245.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000219.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000040.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000060.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000223.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000244.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000111.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000047.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000136.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000099.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000147.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000229.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000050.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000387.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000341.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000030.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000269.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000388.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000239.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000347.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000200.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000355.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000036.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000130.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000044.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000407.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000384.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000273.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000416.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000096.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000395.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000098.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000365.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000066.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000108.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000253.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000080.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000337.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000264.flo (deflated 14%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000230.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000093.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000330.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000039.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000000.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000116.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000118.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000262.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000299.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000135.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000189.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000339.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000090.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000085.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000240.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000187.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000221.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000296.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000392.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000250.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000193.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000332.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000331.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000382.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000184.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000281.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000263.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000248.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000097.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000398.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000134.flo (deflated 14%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000408.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000179.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000033.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000196.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000122.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000379.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000265.flo (deflated 14%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000084.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000402.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000038.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000226.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000170.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000138.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000128.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000026.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000007.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000319.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000351.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000020.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000222.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000285.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000342.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000368.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000064.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000374.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000315.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000029.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000375.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000035.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000203.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000258.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000267.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000125.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000195.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000004.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000329.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000218.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000056.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000348.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000141.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000366.flo (deflated 7%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000032.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000151.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000396.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000129.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000157.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000079.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000197.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000165.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000397.flo (deflated 13%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000335.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000059.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000356.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000231.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000095.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000210.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000216.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000391.flo (deflated 12%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000101.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000191.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000052.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000133.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000204.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000377.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000011.flo (deflated 11%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000061.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000242.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000235.flo (deflated 8%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000385.flo (deflated 10%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000063.flo (deflated 9%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000256.flo (deflated 14%)\n","  adding: content/flownet2-pytorch/results/inference/run.epoch-0-flow-field/000268.flo (deflated 13%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tsg2qcbqIyW8"},"source":["# import shutil\n","\n","# shutil.rmtree('/content/flownet2-pytorch/anon010')\n","# shutil.rmtree('/content/flownet2-pytorch/results')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87b72AFLGF4L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613299642463,"user_tz":-60,"elapsed":500,"user":{"displayName":"alabi tosin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd9E9e_4M1lgxC0quh5EFDDDvItDZB_Q0UvGDLjg=s64","userId":"05932450891233167784"}},"outputId":"2a1e3dae-b1be-4823-d929-0ed57cbab74c"},"source":["2+2"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"K9SeH3KwADMW"},"source":[""],"execution_count":null,"outputs":[]}]}